"""
AI Variable Inspector - å°†Pythonè¿è¡Œæ—¶çŠ¶æ€è½¬åŒ–ä¸ºAIå¯è¯»çš„ç»“æ„åŒ–ä¿¡æ¯
ä¸“é—¨è®¾è®¡ç”¨äºJupyter Notebookç¯å¢ƒï¼Œå¸®åŠ©AIç†è§£EEGåˆ†æä¸­çš„æ•°æ®æµåŠ¨

ç‰¹æ€§ï¼š
    - æ™ºèƒ½æ˜¾ç¤ºç­–ç•¥ï¼šæ ¹æ®æ•°æ®å¤§å°è‡ªåŠ¨è°ƒæ•´æ˜¾ç¤ºè¯¦ç»†ç¨‹åº¦
    - å®Œå…¨ç¡®å®šæ€§ï¼šç›¸åŒè¾“å…¥äº§ç”Ÿç›¸åŒè¾“å‡ºï¼Œé€‚åˆç§‘ç ”ç¯å¢ƒ
    - é›¶æˆæœ¬ï¼šæ— éœ€APIè°ƒç”¨ï¼Œå³æ—¶å“åº”
    - å¯é…ç½®è¯¦ç»†ç¨‹åº¦ï¼šæ”¯æŒ minimal/auto/normal/full å››ç§æ¨¡å¼

ä½¿ç”¨æ–¹æ³•ï¼š
    from utils_EEG.ai_variable_inspector import inspect_for_ai

    # åŸºç¡€ä½¿ç”¨ - è‡ªåŠ¨è°ƒæ•´æ˜¾ç¤ºç­–ç•¥
    epochs = mne.read_epochs(...)
    inspect_for_ai(epochs, name="epochs")

    # æŒ‡å®šè¯¦ç»†ç¨‹åº¦
    inspect_for_ai(large_df, name="results", verbosity="minimal")  # ä»…ç»Ÿè®¡
    inspect_for_ai(small_df, name="metadata", verbosity="full")    # å®Œæ•´æ•°æ®
"""

import numpy as np
import pandas as pd
from typing import Any, Dict, List
from pathlib import Path

# å»¶è¿Ÿå¯¼å…¥mneï¼Œé¿å…åœ¨æ²¡æœ‰mneç¯å¢ƒæ—¶å¯¼å…¥å¤±è´¥
try:
    import mne
    HAS_MNE = True
except ImportError:
    HAS_MNE = False


def inspect_for_ai(var: Any, name: str = "variable", max_depth: int = 2, verbosity: str = "auto") -> None:
    """
    æ‰“å°AI agentéœ€è¦çš„å˜é‡ç»“æ„åŒ–ä¿¡æ¯

    Parameters
    ----------
    var : Any
        è¦æ£€æŸ¥çš„å˜é‡
    name : str
        å˜é‡åç§°ï¼ˆç”¨äºè¾“å‡ºæ ‡è¯†ï¼‰
    max_depth : int
        åµŒå¥—å­—å…¸/å¯¹è±¡çš„æœ€å¤§æ£€æŸ¥æ·±åº¦
    verbosity : str
        è¯¦ç»†ç¨‹åº¦æ§åˆ¶ ("auto", "minimal", "normal", "full")
        - "auto": æ ¹æ®æ•°æ®å¤§å°è‡ªåŠ¨è°ƒæ•´ï¼ˆé»˜è®¤ï¼‰
        - "minimal": æœ€å°‘ä¿¡æ¯ï¼Œä»…æ˜¾ç¤ºç»Ÿè®¡æ‘˜è¦
        - "normal": æ ‡å‡†ä¿¡æ¯
        - "full": å®Œæ•´ä¿¡æ¯ï¼Œæ˜¾ç¤ºæ‰€æœ‰æ•°æ®
    """
    print(f"\n{'='*80}")
    print(f"ğŸ¤– AI VARIABLE INSPECTION: {name}")
    print(f"{'='*80}\n")

    # 1. åŸºç¡€ç±»å‹ä¿¡æ¯
    print(f"ğŸ“Œ Type: {type(var).__module__}.{type(var).__name__}")
    print(f"ğŸ“Œ Memory: {_get_size_mb(var):.2f} MB")

    # 2. æ ¹æ®ç±»å‹åˆ†å‘æ£€æŸ¥é€»è¾‘
    if isinstance(var, (np.ndarray, list, tuple)):
        _inspect_array_like(var, name, verbosity)
    elif isinstance(var, pd.DataFrame):
        _inspect_dataframe(var, name, verbosity)
    elif isinstance(var, dict):
        _inspect_dict(var, name, max_depth, verbosity=verbosity)
    elif _is_mne_object(var):
        _inspect_mne_object(var, name)
    elif hasattr(var, '__dict__'):
        _inspect_custom_object(var, name, max_depth)
    else:
        _inspect_primitive(var, name)

    # 3. æ•°æ®æµæç¤º
    print(f"\nğŸ’¡ AI Usage Hints:")
    _print_usage_hints(var, name)

    print(f"\n{'='*80}\n")


def _get_size_mb(obj: Any) -> float:
    """ä¼°ç®—å¯¹è±¡å†…å­˜å ç”¨"""
    try:
        if isinstance(obj, np.ndarray):
            return obj.nbytes / (1024**2)
        elif isinstance(obj, pd.DataFrame):
            return obj.memory_usage(deep=True).sum() / (1024**2)
        else:
            return 0.0  # å…¶ä»–ç±»å‹æš‚ä¸ç²¾ç¡®è®¡ç®—
    except:
        return 0.0


def _inspect_array_like(var: Any, name: str, verbosity: str = "auto") -> None:
    """æ£€æŸ¥æ•°ç»„ç±»å¯¹è±¡ - æ™ºèƒ½é‡‡æ ·ç­–ç•¥"""
    if isinstance(var, np.ndarray):
        arr = var
    else:
        arr = np.array(var) if len(var) > 0 else np.array([])

    print(f"\nğŸ“Š Array Structure:")
    print(f"  Shape: {arr.shape}")
    print(f"  Dtype: {arr.dtype}")
    print(f"  Dimensions: {arr.ndim}D")
    print(f"  Total elements: {arr.size}")

    if arr.size == 0:
        print(f"  (Empty array)")
        return

    # æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å€¼ç±»å‹
    is_numeric = np.issubdtype(arr.dtype, np.number)

    if is_numeric:
        # åŸºç¡€ç»Ÿè®¡
        print(f"\n  ç»Ÿè®¡ä¿¡æ¯:")
        print(f"    Range: [{np.min(arr):.4f}, {np.max(arr):.4f}]")
        print(f"    Mean: {np.mean(arr):.4f}, Std: {np.std(arr):.4f}")

        # æ ¹æ®å¤§å°æ˜¾ç¤ºä¸åŒè¯¦ç»†ç¨‹åº¦
        if arr.size > 10000 or verbosity == "full":
            # å¤§æ•°ç»„ï¼šæ·»åŠ åˆ†ä½æ•°
            print(f"    Quantiles:")
            print(f"      25%: {np.percentile(arr, 25):.4f}")
            print(f"      50%: {np.percentile(arr, 50):.4f}")
            print(f"      75%: {np.percentile(arr, 75):.4f}")
    else:
        print(f"  (Non-numeric array, skipping statistics)")

    # é‡‡æ ·æ˜¾ç¤º
    print(f"\n  é‡‡æ ·å€¼:")
    if arr.size <= 20 or verbosity == "full":
        # å°æ•°ç»„ï¼šæ˜¾ç¤ºå…¨éƒ¨
        print(f"    All values: {arr.flat[:20]}")
    elif arr.size <= 100:
        # ä¸­ç­‰æ•°ç»„ï¼šæ˜¾ç¤ºå¤´å°¾
        print(f"    First 5: {arr.flat[:5]}")
        print(f"    Last 5: {arr.flat[-5:]}")
    else:
        # å¤§æ•°ç»„ï¼šç¨€ç–é‡‡æ ·
        print(f"    First 3: {arr.flat[:3]}")
        print(f"    Middle 3: {arr.flat[arr.size//2-1:arr.size//2+2]}")
        print(f"    Last 3: {arr.flat[-3:]}")


def _inspect_dataframe(df: pd.DataFrame, name: str, verbosity: str = "auto") -> None:
    """æ£€æŸ¥DataFrame - æ™ºèƒ½æ˜¾ç¤ºç­–ç•¥"""
    print(f"\nğŸ“Š DataFrame Structure:")
    print(f"  Shape: {df.shape} (rows Ã— columns)")
    print(f"  Columns: {list(df.columns)}")

    # æ•°æ®ç±»å‹ä¿¡æ¯
    print(f"\n  æ•°æ®ç±»å‹:")
    print(f"    {df.dtypes.to_string(max_rows=10)}")

    # ç¼ºå¤±å€¼ç»Ÿè®¡
    missing = df.isnull().sum()
    if missing.sum() > 0:
        print(f"\n  ç¼ºå¤±å€¼: {missing[missing > 0].to_dict()}")
    else:
        print(f"\n  ç¼ºå¤±å€¼: æ— ")

    # æ ¹æ®å¤§å°æ™ºèƒ½æ˜¾ç¤ºæ•°æ®
    n_rows = df.shape[0]

    if verbosity == "minimal":
        # æœ€å°æ¨¡å¼ï¼šä»…æ˜¾ç¤ºç»Ÿè®¡
        print(f"\n  æ•°æ®æ‘˜è¦:")
        print(df.describe().to_string())

    elif n_rows <= 10 or verbosity == "full":
        # å°è¡¨æ ¼æˆ–å®Œæ•´æ¨¡å¼ï¼šæ˜¾ç¤ºå…¨éƒ¨æ•°æ®
        print(f"\n  å®Œæ•´æ•°æ®:")
        print(df.to_string())

    elif n_rows <= 100:
        # ä¸­ç­‰è¡¨æ ¼ï¼šæ˜¾ç¤ºå¤´å°¾
        print(f"\n  å‰5è¡Œ:")
        print(df.head(5).to_string())
        print(f"\n  å5è¡Œ:")
        print(df.tail(5).to_string())

        # æ•°å€¼åˆ—çš„ç»Ÿè®¡æ‘˜è¦
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            print(f"\n  æ•°å€¼åˆ—ç»Ÿè®¡:")
            print(df[numeric_cols].describe().to_string())

    else:
        # å¤§è¡¨æ ¼ï¼šç»Ÿè®¡ + é‡‡æ ·
        print(f"\n  æ•°æ®æ‘˜è¦:")
        print(df.describe().to_string())

        print(f"\n  å‰3è¡Œ:")
        print(df.head(3).to_string())

        # éšæœºé‡‡æ ·ï¼ˆå›ºå®šç§å­ä¿è¯å¯é‡å¤æ€§ï¼‰
        print(f"\n  éšæœºé‡‡æ ·3è¡Œ (ç§å­=42):")
        sample_size = min(3, len(df))
        print(df.sample(n=sample_size, random_state=42).to_string())


def _inspect_dict(d: dict, name: str, max_depth: int, _current_depth: int = 0, verbosity: str = "auto") -> None:
    """æ£€æŸ¥å­—å…¸ - æ™ºèƒ½å±•å¼€ç­–ç•¥"""
    print(f"\nğŸ“¦ Dictionary Structure (depth {_current_depth}):")

    n_keys = len(d)
    print(f"  æ€»é”®æ•°: {n_keys}")

    # æ ¹æ®å­—å…¸å¤§å°å†³å®šæ˜¾ç¤ºç­–ç•¥
    if n_keys <= 20 or verbosity == "full":
        print(f"  æ‰€æœ‰é”®: {list(d.keys())}")
    else:
        print(f"  å‰10ä¸ªé”®: {list(d.keys())[:10]}")
        print(f"  å10ä¸ªé”®: {list(d.keys())[-10:]}")

    # ç»Ÿè®¡å€¼ç±»å‹åˆ†å¸ƒ
    type_counts = {}
    for value in d.values():
        vtype = type(value).__name__
        type_counts[vtype] = type_counts.get(vtype, 0) + 1
    print(f"\n  å€¼ç±»å‹åˆ†å¸ƒ: {type_counts}")

    # æ™ºèƒ½å±•å¼€é‡è¦çš„é”®å€¼å¯¹
    print(f"\n  é”®å€¼è¯¦æƒ…:")
    items_to_show = list(d.items())[:20] if n_keys > 20 else list(d.items())

    for key, value in items_to_show:
        value_type = type(value).__name__

        if isinstance(value, np.ndarray):
            shape = value.shape
            dtype = value.dtype
            print(f"    '{key}': {value_type} {shape} {dtype}")

            # å¯¹äºé‡è¦çš„æ•°ç»„ï¼Œæ˜¾ç¤ºç®€è¦ç»Ÿè®¡
            if value.size > 0 and np.issubdtype(value.dtype, np.number):
                print(f"      â†’ Range: [{np.min(value):.4f}, {np.max(value):.4f}]")

        elif isinstance(value, list):
            print(f"    '{key}': {value_type} len={len(value)}")
            if len(value) > 0:
                first_elem = value[0]
                first_type = type(first_elem).__name__
                print(f"      â†’ First element type: {first_type}")

                # å¦‚æœæ˜¯ DataFrame listï¼Œæ˜¾ç¤ºæ›´å¤šç»†èŠ‚
                if isinstance(first_elem, pd.DataFrame):
                    print(f"      â†’ Sample DataFrame [0]:")
                    print(f"         Shape: {first_elem.shape}")

                    # æ˜¾ç¤ºæ‰€æœ‰åˆ—åï¼ˆå¦‚æœåˆ—æ•°è¾ƒå¤šï¼Œåˆ†è¡Œæ˜¾ç¤ºï¼‰
                    cols = list(first_elem.columns)
                    if len(cols) <= 10:
                        print(f"         Columns: {cols}")
                    else:
                        print(f"         Columns ({len(cols)} total):")
                        # æ¯è¡Œæ˜¾ç¤º5ä¸ªåˆ—å
                        for i in range(0, len(cols), 5):
                            chunk = cols[i:i+5]
                            print(f"           {chunk}")

                    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ DataFrame ç»“æ„ç›¸åŒ
                    if len(value) > 1 and all(isinstance(v, pd.DataFrame) for v in value[:min(10, len(value))]):
                        shapes = [v.shape for v in value[:min(10, len(value))]]
                        all_same = all(s == shapes[0] for s in shapes)
                        print(f"         All DataFrames same structure: {all_same}")
                        if not all_same:
                            print(f"         Shape variations: {set(shapes)}")

                # å¦‚æœæ˜¯ ndarray listï¼Œæ˜¾ç¤ºæ›´å¤šç»†èŠ‚
                elif isinstance(first_elem, np.ndarray):
                    print(f"      â†’ Sample array [0]: shape {first_elem.shape}, dtype {first_elem.dtype}")
                    if len(value) > 1:
                        shapes = [v.shape for v in value[:min(10, len(value))] if isinstance(v, np.ndarray)]
                        all_same = all(s == shapes[0] for s in shapes)
                        print(f"         All arrays same shape: {all_same}")
                        if not all_same:
                            print(f"         Shape variations: {set(shapes)}")

        elif isinstance(value, pd.DataFrame):
            print(f"    '{key}': DataFrame {value.shape}")
            print(f"      â†’ Columns: {list(value.columns)[:5]}{'...' if len(value.columns) > 5 else ''}")

            # æ˜¾ç¤ºæ•°æ®ç±»å‹åˆ†å¸ƒ
            dtype_counts = value.dtypes.value_counts().to_dict()
            print(f"      â†’ Dtypes: {dtype_counts}")

            # æ˜¾ç¤ºç¼ºå¤±å€¼æƒ…å†µ
            missing_count = value.isnull().sum().sum()
            if missing_count > 0:
                print(f"      â†’ Missing values: {missing_count} total")

            # æ˜¾ç¤ºæ•°æ®é¢„è§ˆï¼ˆå‰2è¡Œï¼‰
            if verbosity != "minimal" and value.shape[0] > 0:
                print(f"      â†’ Preview (first 2 rows):")
                preview = value.head(2).to_string(max_cols=5, max_colwidth=20)
                for line in preview.split('\n'):
                    print(f"         {line}")

        elif isinstance(value, dict) and _current_depth < max_depth:
            print(f"    '{key}': dict with {len(value)} items")
            # é€’å½’å±•å¼€åµŒå¥—å­—å…¸
            _inspect_dict(value, f"{name}['{key}']", max_depth, _current_depth + 1, verbosity)

        else:
            # å…¶ä»–ç±»å‹
            if isinstance(value, (int, float, str, bool)) and verbosity != "minimal":
                print(f"    '{key}': {value_type} = {value}")
            else:
                print(f"    '{key}': {value_type}")

    if n_keys > 20 and verbosity != "full":
        print(f"  ... (çœç•¥ {n_keys - 20} ä¸ªé”®)")


def _is_mne_object(obj: Any) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºMNEå¯¹è±¡"""
    if not HAS_MNE:
        return False
    return any(base.__module__.startswith('mne') for base in type(obj).__mro__)


def _inspect_mne_object(obj: Any, name: str) -> None:
    """æ£€æŸ¥MNEå¯¹è±¡ï¼ˆEpochs, Evoked, Rawç­‰ï¼‰"""
    print(f"\nğŸ§  MNE Object Structure:")

    # é€šç”¨MNEå±æ€§
    if hasattr(obj, 'info'):
        info = obj.info
        print(f"  Channels: {len(info['ch_names'])} ({info['ch_names'][:5]}...)")
        print(f"  Sampling rate: {info['sfreq']} Hz")

    # Epochsç‰¹æœ‰
    if hasattr(obj, 'events'):
        print(f"  Events: {len(obj.events)} trials")
        print(f"  Event IDs: {obj.event_id}")
        print(f"  Time range: [{obj.tmin}, {obj.tmax}] sec")
        if hasattr(obj, '_data'):
            print(f"  Data shape: {obj._data.shape} (epochs Ã— channels Ã— timepoints)")

    # Evokedç‰¹æœ‰
    elif hasattr(obj, 'nave'):
        print(f"  Averaged trials: {obj.nave}")
        print(f"  Time range: [{obj.times[0]:.3f}, {obj.times[-1]:.3f}] sec")
        print(f"  Data shape: {obj.data.shape} (channels Ã— timepoints)")

    # Connectivityç»“æœ
    elif hasattr(obj, 'get_data'):
        try:
            data = obj.get_data()
            print(f"  Data shape: {data.shape}")
            print(f"  Method: {obj.method if hasattr(obj, 'method') else 'unknown'}")
        except:
            pass

    # Metadata
    if hasattr(obj, 'metadata') and obj.metadata is not None:
        print(f"  Metadata: {obj.metadata.shape[1]} columns")
        print(f"    Columns: {list(obj.metadata.columns)}")


def _inspect_custom_object(obj: Any, name: str, max_depth: int) -> None:
    """æ£€æŸ¥è‡ªå®šä¹‰å¯¹è±¡"""
    print(f"\nğŸ”§ Custom Object Attributes:")

    attrs = {k: v for k, v in obj.__dict__.items() if not k.startswith('_')}

    for attr_name, attr_value in list(attrs.items())[:10]:  # é™åˆ¶è¾“å‡ºæ•°é‡
        attr_type = type(attr_value).__name__

        if isinstance(attr_value, np.ndarray):
            print(f"  {attr_name}: {attr_type} {attr_value.shape}")
        elif isinstance(attr_value, (list, tuple)):
            print(f"  {attr_name}: {attr_type} len={len(attr_value)}")
        else:
            print(f"  {attr_name}: {attr_type}")


def _inspect_primitive(var: Any, name: str) -> None:
    """æ£€æŸ¥åŸºç¡€ç±»å‹"""
    print(f"\nğŸ“ Value: {var}")


def _print_usage_hints(var: Any, name: str) -> None:
    """æ ¹æ®å˜é‡ç±»å‹æä¾›AIä½¿ç”¨å»ºè®®"""
    var_type = type(var).__name__

    if isinstance(var, np.ndarray):
        print(f"  - Access data: {name}[index] or {name}.flatten()")
        print(f"  - Shape manipulation: {name}.reshape(...)")

    elif isinstance(var, pd.DataFrame):
        print(f"  - Access columns: {name}['column_name']")
        print(f"  - Filter rows: {name}[{name}['col'] > value]")
        print(f"  - Groupby: {name}.groupby('col').mean()")

    elif isinstance(var, dict):
        print(f"  - Access values: {name}['key']")
        print(f"  - Iterate: for k, v in {name}.items()")

    elif _is_mne_object(var):
        if hasattr(var, 'get_data'):
            print(f"  - Extract data: {name}.get_data()")
        if hasattr(var, 'crop'):
            print(f"  - Crop time: {name}.crop(tmin, tmax)")
        if hasattr(var, 'apply_baseline'):
            print(f"  - Baseline: {name}.apply_baseline((tmin, tmax))")
        if hasattr(var, 'metadata'):
            print(f"  - Filter by metadata: {name}[{name}.metadata['condition'] == 'M']")


# ============================================================================
# æ‰¹é‡æ£€æŸ¥å·¥å…·
# ============================================================================

def batch_inspect(variables: Dict[str, Any], max_depth: int = 2) -> None:
    """
    æ‰¹é‡æ£€æŸ¥å¤šä¸ªå˜é‡

    Parameters
    ----------
    variables : dict
        {'å˜é‡å': å˜é‡å€¼} çš„å­—å…¸
    max_depth : int
        æ£€æŸ¥æ·±åº¦

    Example
    -------
    >>> batch_inspect({
    ...     'epochs': epochs,
    ...     'connectivity': conn_results,
    ...     'behavior_df': df
    ... })
    """
    for name, var in variables.items():
        inspect_for_ai(var, name=name, max_depth=max_depth)


# ============================================================================
# æ•°æ®æµè¿½è¸ªè£…é¥°å™¨
# ============================================================================

def track_data_flow(func):
    """
    è£…é¥°å™¨ï¼šè‡ªåŠ¨è¿½è¸ªå‡½æ•°è¾“å…¥è¾“å‡º

    Example
    -------
    >>> @track_data_flow
    ... def process_epochs(epochs):
    ...     return epochs.crop(0, 1)
    """
    def wrapper(*args, **kwargs):
        print(f"\nğŸ”„ FUNCTION CALL: {func.__name__}")
        print(f"{'='*80}")

        # è¾“å…¥æ£€æŸ¥
        print("ğŸ“¥ INPUTS:")
        for i, arg in enumerate(args):
            inspect_for_ai(arg, name=f"arg{i}")
        for key, val in kwargs.items():
            inspect_for_ai(val, name=key)

        # æ‰§è¡Œå‡½æ•°
        result = func(*args, **kwargs)

        # è¾“å‡ºæ£€æŸ¥
        print("\nğŸ“¤ OUTPUT:")
        inspect_for_ai(result, name="return_value")

        return result

    return wrapper


# ============================================================================
# Notebookç¯å¢ƒå¿«æ·å‡½æ•°
# ============================================================================

def quick_check(*vars_with_names):
    """
    å¿«é€Ÿæ£€æŸ¥å¤šä¸ªå˜é‡ï¼ˆç®€åŒ–ç‰ˆï¼‰

    Example
    -------
    >>> quick_check(
    ...     ('epochs', epochs),
    ...     ('df', behavior_df)
    ... )
    """
    for name, var in vars_with_names:
        print(f"\nğŸ” {name}: {type(var).__name__}", end="")

        if isinstance(var, np.ndarray):
            print(f" {var.shape} {var.dtype}")
        elif isinstance(var, pd.DataFrame):
            print(f" {var.shape}")
        elif hasattr(var, '_data'):
            print(f" {var._data.shape}")
        else:
            print()


if __name__ == "__main__":
    # æµ‹è¯•ç¤ºä¾‹ - å±•ç¤ºæ™ºèƒ½æ˜¾ç¤ºç­–ç•¥
    print("=" * 80)
    print("AI Variable Inspector - Enhanced Test Mode")
    print("=" * 80)

    # æµ‹è¯•1: å°æ•°ç»„ - æ˜¾ç¤ºå®Œæ•´ä¿¡æ¯
    print("\n\nã€æµ‹è¯•1ã€‘å°æ•°ç»„ (è‡ªåŠ¨æ˜¾ç¤ºå®Œæ•´)")
    small_array = np.array([1, 2, 3, 4, 5])
    inspect_for_ai(small_array, name="small_array")

    # æµ‹è¯•2: å¤§æ•°ç»„ - æ™ºèƒ½é‡‡æ ·
    print("\n\nã€æµ‹è¯•2ã€‘å¤§æ•°ç»„ (æ™ºèƒ½é‡‡æ ·)")
    large_array = np.random.randn(10, 64, 500)
    inspect_for_ai(large_array, name="eeg_data")

    # æµ‹è¯•3: å°DataFrame - æ˜¾ç¤ºå®Œæ•´
    print("\n\nã€æµ‹è¯•3ã€‘å°DataFrame (æ˜¾ç¤ºå®Œæ•´)")
    small_df = pd.DataFrame({
        'subject': ['pre001', 'pre002'],
        'condition': ['M', 'S'],
        'accuracy': [0.85, 0.92]
    })
    inspect_for_ai(small_df, name="small_behavior_data")

    # æµ‹è¯•4: å¤§DataFrame - ç»Ÿè®¡+é‡‡æ ·
    print("\n\nã€æµ‹è¯•4ã€‘å¤§DataFrame (ç»Ÿè®¡+é‡‡æ ·)")
    large_df = pd.DataFrame({
        'subject': [f'pre{i:03d}' for i in range(200)],
        'condition': ['M', 'S'] * 100,
        'accuracy': np.random.rand(200),
        'rt': np.random.randn(200) * 100 + 500
    })
    inspect_for_ai(large_df, name="large_behavior_data")

    # æµ‹è¯•5: å­—å…¸ - æ™ºèƒ½å±•å¼€
    print("\n\nã€æµ‹è¯•5ã€‘å­—å…¸ (æ™ºèƒ½å±•å¼€)")
    test_dict = {
        'connectivity': np.random.randn(64, 64),
        'freqs': np.linspace(4, 30, 27),
        'times': np.linspace(-0.5, 1.5, 500),
        'metadata': {'n_subjects': 10, 'condition': 'M'}
    }
    inspect_for_ai(test_dict, name="analysis_results")

    # æµ‹è¯•6: verbosityå‚æ•°
    print("\n\nã€æµ‹è¯•6ã€‘verbosity='minimal' (ä»…ç»Ÿè®¡)")
    inspect_for_ai(large_df, name="large_df_minimal", verbosity="minimal")

    print("\n\nã€æµ‹è¯•6ã€‘verbosity='full' (å®Œæ•´æ•°æ®)")
    inspect_for_ai(small_df, name="small_df_full", verbosity="full")

    print("\n\n" + "=" * 80)
    print("æµ‹è¯•å®Œæˆï¼æ‰€æœ‰åŠŸèƒ½æ­£å¸¸å·¥ä½œã€‚")
    print("=" * 80)
