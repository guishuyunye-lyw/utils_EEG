{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T23:50:23.453475Z",
     "start_time": "2025-01-18T23:50:22.377976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "just for testing \n",
    "\n",
    "the parallel version seem not work well here.\n",
    "\n",
    "run the permutation in utils_EEG/utils_tfcePermu.py\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import mne\n",
    "import mne.stats.cluster_level_backup as cluster_level_backup\n",
    "from importlib import reload\n",
    "import utils_EEG.stuff as stuff\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from mne.channels import find_ch_adjacency\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Reload custom module to ensure latest changes\n",
    "reload(stuff)\n",
    "\n",
    "# Define the set of channels to use (excluding FT9, FT10, TP9, TP10)\n",
    "channels_all = [\n",
    "    'Fp1', 'Fp2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4',\n",
    "    'O1', 'O2', 'F7', 'F8', 'T7', 'T8', 'P7', 'P8',\n",
    "    'Fz', 'Cz', 'Pz', 'FC1', 'FC2', 'CP1', 'CP2',\n",
    "    'FC5', 'FC6', 'CP5', 'CP6', 'F1', 'F2', 'C1',\n",
    "    'C2', 'P1', 'P2', 'AF3', 'AF4', 'FC3', 'FC4',\n",
    "    'CP3', 'CP4', 'PO3', 'PO4', 'F5', 'F6', 'C5',\n",
    "    'C6', 'P5', 'P6', 'AF7', 'AF8', 'FT7', 'FT8',\n",
    "    'TP7', 'TP8', 'PO7', 'PO8', 'Fpz', 'CPz', 'POz', 'Oz'\n",
    "]\n",
    "\n",
    "# Load an evoked file as an example\n",
    "file_path = r\"D:\\LYW\\pre10\\data\\7evoked_allWords\\prex006-ave.fif\"\n",
    "evoked_example = mne.read_evokeds(file_path, proj=False, verbose=None)\n",
    "evoked_example = evoked_example[0].resample(91)  # Resample to 91 Hz\n",
    "evoked_example.pick_channels(channels_all)  # Select a subset of channels\n",
    "\n",
    "# Compute adjacency for the chosen channels\n",
    "info = evoked_example.info\n",
    "adjacency, ch_names = find_ch_adjacency(info, ch_type=\"eeg\")\n",
    "\n",
    "# Set parameters related to cluster analysis\n",
    "max_step = 1\n",
    "t_power = 1\n",
    "n_tests = 59 * 91\n",
    "n_times = 91\n",
    "\n",
    "# Setup adjacency structure required for TFCE calculations\n",
    "adjacency2 = cluster_level_backup._setup_adjacency(adjacency, n_tests, n_times)\n",
    "\n",
    "# Directory containing the CSV files for permutation summary\n",
    "csv_dir = r\"D:\\LYW\\pre10\\data\\permutation_summary_between\"\n",
    "# csv_files = glob.glob(os.path.join(csv_dir, \"summary_results_permutation_*.csv\"))\n",
    "# exclude updated clusters\n",
    "csv_files = glob.glob(os.path.join(csv_dir, \"summary_results_permutation_[0-9]*.csv\"))\n",
    "# Filter out anything that contains \"_updated\"\n",
    "csv_files = [f for f in csv_files \n",
    "             if \"_updated\" not in os.path.basename(f) \n",
    "             and \"updated_clustersT\" not in os.path.basename(f)]\n",
    "\n",
    "# TFCE threshold parameters\n",
    "threshold_tfce = {\n",
    "    \"start\": 0,\n",
    "    \"step\": 0.001,\n",
    "    \"h_power\": 2,\n",
    "    \"e_power\": 0.5\n",
    "}\n",
    "\n",
    "# Tail parameter (set here to 1, meaning massed < spaced)\n",
    "tail = 1\n",
    "\n",
    "\n",
    "# For debugging\n",
    "print(f\"adjacency2: {type(adjacency2)}\")\n",
    "print(f\"n_tests: {type(n_tests)} - {n_tests}\")\n",
    "print(f\"n_times: {type(n_times)} - {n_times}\")\n",
    "print(f\"threshold_tfce: {type(threshold_tfce)} - {threshold_tfce}\")\n"
   ],
   "id": "72913fdc1ce449cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reload ok\n",
      "reload stuff module\n",
      "D:\\ProgramData\\anaconda3\\envs\\mne12\\python.exe\n",
      "导入成功\n",
      "reload stuff module\n",
      "D:\\ProgramData\\anaconda3\\envs\\mne12\\python.exe\n",
      "导入成功\n",
      "Reading D:\\LYW\\pre10\\data\\7evoked_allWords\\prex006-ave.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     800.00 ms (0.40 × 100 + 0.20 × 101 + 0.20 × 102 + 0.20 × 103)\n",
      "        0 CTF compensation matrices available\n",
      "        nave = 250 - aspect type = 100\n",
      "Loaded Evoked data is baseline-corrected (baseline: [-0.2, 0] s)\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Could not find a adjacency matrix for the data. Computing adjacency based on Delaunay triangulations.\n",
      "-- number of adjacent vertices : 59\n",
      "adjacency2: <class 'list'>\n",
      "n_tests: <class 'int'> - 5369\n",
      "n_times: <class 'int'> - 91\n",
      "threshold_tfce: <class 'dict'> - {'start': 0, 'step': 0.001, 'h_power': 2, 'e_power': 0.5}\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T23:50:23.484475Z",
     "start_time": "2025-01-18T23:50:23.463475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "read the csv file including coef and std\n",
    "so you need to calculate and smooth the t values using the coef and std.\n",
    "then correct the t to tfce.\n",
    "'''\n",
    "def process_csv_file(csv_file_path, adjacency2, test_iterations, sigma, threshold_tfce, max_step, t_power):\n",
    "    \"\"\"\n",
    "    Process a single CSV file:\n",
    "    1. Compute permutation-based t-values using apply_lmer_models_to_array_parallel_hat.\n",
    "    2. Compute TFCE  for both t_obs and -t_obs.\n",
    "    \"\"\"\n",
    "    print(f\"Processing {csv_file_path}\")\n",
    "    print(f\"csv_file_path: {type(csv_file_path)} - {csv_file_path}\")\n",
    "    print(f\"n_tests: {type(n_tests)} - {n_tests}\")\n",
    "    print(f\"n_times: {type(n_times)} - {n_times}\")\n",
    "    print(f\"threshold_tfce: {type(threshold_tfce)} - {threshold_tfce}\")\n",
    "\n",
    "    # Compute permutation-based t-values\n",
    "    # this version just load coef and std from csv file and don't fit model\n",
    "    t_obs_orig_permu = stuff.apply_lmer_models_to_array_parallel_hat(\n",
    "        array_of_dfs=None,\n",
    "        test_iterations=test_iterations,\n",
    "        max_workers=5,#useless\n",
    "        sigma=sigma,\n",
    "        csv_file_path=csv_file_path#\n",
    "    )\n",
    "\n",
    "    # Calculate max and min values of t_obs_orig_permu\n",
    "    max_t_obs = np.max(t_obs_orig_permu)\n",
    "    min_t_obs = np.min(t_obs_orig_permu)\n",
    "    # print the max and min values of t_obs_orig_permu\n",
    "    print(f\"Max value of t_obs_orig_permu: {max_t_obs}\")\n",
    "    print(f\"Min value of t_obs_orig_permu: {min_t_obs}\")\n",
    "\n",
    "    # Define number of steps\n",
    "    # use same number of steps rather than same step size.\n",
    "    num_steps = cluster_level_backup.num_steps\n",
    "\n",
    "    # Calculate step for positive and negative threshold_tfce\n",
    "    step_positive = max_t_obs / num_steps\n",
    "    step_negative = abs(min_t_obs) / num_steps\n",
    "\n",
    "    # Update threshold_tfce for positive and negative directions\n",
    "    threshold_tfce_positive = {\n",
    "        \"start\": 0,\n",
    "        \"step\": step_positive,\n",
    "        \"h_power\": 2,\n",
    "        \"e_power\": 0.5\n",
    "    }\n",
    "\n",
    "    threshold_tfce_negative = {\n",
    "        \"start\": 0,\n",
    "        \"step\": step_negative,\n",
    "        \"h_power\": 2,\n",
    "        \"e_power\": 0.5\n",
    "    }\n",
    "\n",
    "    print(f\"Step for positive threshold_tfce: {step_positive}\")\n",
    "    print(f\"Step for negative threshold_tfce: {step_negative}\")\n",
    "\n",
    "    # Find clusters for the positive side\n",
    "    out = cluster_level_backup._find_clusters(\n",
    "        x=t_obs_orig_permu,\n",
    "        threshold=threshold_tfce_positive,\n",
    "        tail=1,\n",
    "        adjacency=adjacency2,\n",
    "        max_step=max_step,\n",
    "        include=None,\n",
    "        partitions=None,\n",
    "        t_power=t_power,\n",
    "        show_info=True,\n",
    "    )\n",
    "\n",
    "    # Find clusters for the negative side\n",
    "    out2 = cluster_level_backup._find_clusters(\n",
    "        x=-t_obs_orig_permu, # transfer the sign of t_obs_orig_permu\n",
    "        threshold=threshold_tfce_negative,\n",
    "        tail=1,\n",
    "        adjacency=adjacency2,\n",
    "        max_step=max_step,\n",
    "        include=None,\n",
    "        partitions=None,\n",
    "        t_power=t_power,\n",
    "        show_info=True,\n",
    "    )\n",
    "\n",
    "    return csv_file_path, {'out': out, 'out2': out2}\n",
    "\n",
    "'''\n",
    "\n",
    "Calculate H0 distribution of tfce for all permutation.\n",
    "\n",
    "procedure:\n",
    "\n",
    "1. load all csv files\n",
    "2. calculate tfce for all csv files\n",
    "3. find the max and min values of tfce\n",
    "\n",
    "\n",
    "'''\n"
   ],
   "id": "73c80495fa17999c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nCalculate H0 distribution of tfce for all permutation.\\n\\nprocedure:\\n\\n1. load all csv files\\n2. calculate tfce for all csv files\\n3. find the max and min values of tfce\\n\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## test the process of permutation\n",
    "especially the tfce calculation.\n",
    "\n",
    "the def _find_clusters function."
   ],
   "id": "2713008df5dc373e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from importlib import reload\n",
    "import mne.stats.cluster_level_backup as cluster_level_backup\n",
    "reload(cluster_level_backup)\n",
    "\n",
    "csv_file_path = 'D:\\\\LYW\\\\pre10\\\\data\\\\permutation_summary_between\\\\summary_results_permutation_115.csv'\n",
    "\n",
    "# Parameters\n",
    "max_step = 1\n",
    "t_power = 1\n",
    "n_tests = 59 * 91\n",
    "n_times = 91\n",
    "# file_limit = 3 # Number of files to process 490\n",
    "# max_workers = 1\n",
    "file_limit = len(csv_files)\n",
    "max_workers = 10\n",
    "\n",
    "num_steps = cluster_level_backup.num_steps\n",
    "\n",
    "\n",
    "# Compute permutation-based t-values\n",
    "# this version just load coef and std from csv file and don't fit model\n",
    "t_obs_orig_permu = stuff.apply_lmer_models_to_array_parallel_hat(\n",
    "array_of_dfs=None,\n",
    "test_iterations=5555,\n",
    "max_workers=5,#useless\n",
    "sigma=0.001,\n",
    "csv_file_path=csv_file_path#\n",
    ")\n",
    "\n",
    "# Calculate max and min values of t_obs_orig_permu\n",
    "max_t_obs = np.max(t_obs_orig_permu)\n",
    "min_t_obs = np.min(t_obs_orig_permu)\n",
    "# print the max and min values of t_obs_orig_permu\n",
    "print(f\"Max value of t_obs_orig_permu: {max_t_obs}\")\n",
    "print(f\"Min value of t_obs_orig_permu: {min_t_obs}\")\n",
    "\n",
    "# Define number of steps\n",
    "# use same number of steps rather than same step size.\n",
    "num_steps = cluster_level_backup.num_steps\n",
    "\n",
    "# Calculate step for positive and negative threshold_tfce\n",
    "step_positive = max_t_obs / num_steps\n",
    "step_negative = abs(min_t_obs) / num_steps\n",
    "\n",
    "# Update threshold_tfce for positive and negative directions\n",
    "threshold_tfce_positive = {\n",
    "\"start\": 0,\n",
    "\"step\": step_positive,\n",
    "\"h_power\": 2,\n",
    "\"e_power\": 0.5\n",
    "}\n",
    "\n",
    "threshold_tfce_negative = {\n",
    "\"start\": 0,\n",
    "\"step\": step_negative,\n",
    "\"h_power\": 2,\n",
    "\"e_power\": 0.5\n",
    "}\n",
    "\n",
    "print(f\"Step for positive threshold_tfce: {step_positive}\")\n",
    "print(f\"Step for negative threshold_tfce: {step_negative}\")\n",
    "\n",
    "# Find clusters for the positive side\n",
    "out = cluster_level_backup._find_clusters(\n",
    "x=t_obs_orig_permu,\n",
    "threshold=threshold_tfce_positive,\n",
    "tail=1,\n",
    "adjacency=adjacency2,\n",
    "max_step=max_step,\n",
    "include=None,\n",
    "partitions=None,\n",
    "t_power=t_power,\n",
    "show_info=True,\n",
    ")\n",
    "\n",
    "# # Find clusters for the negative side\n",
    "# out2 = cluster_level_backup._find_clusters(\n",
    "# x=-t_obs_orig_permu, # transfer the sign of t_obs_orig_permu\n",
    "# threshold=threshold_tfce_negative,\n",
    "# tail=1,\n",
    "# adjacency=adjacency2,\n",
    "# max_step=max_step,\n",
    "# include=None,\n",
    "# partitions=None,\n",
    "# t_power=t_power,\n",
    "# show_info=True,\n",
    "# )\n",
    "\n"
   ],
   "id": "85a5ea8daa91b3c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T23:50:10.824971300Z",
     "start_time": "2025-01-18T23:49:04.080125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "'''\n",
    "\n",
    "I don't know why \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "results = {}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Reload custom module once more if needed\n",
    "    reload(stuff)\n",
    "\n",
    "    # Parameters\n",
    "    max_step = 1\n",
    "    t_power = 1\n",
    "    n_tests = 59 * 91\n",
    "    n_times = 91\n",
    "    # file_limit = 3 # Number of files to process 490\n",
    "    # max_workers = 1\n",
    "    file_limit = len(csv_files)\n",
    "    max_workers = 8\n",
    "    \n",
    "    num_steps = cluster_level_backup.num_steps\n",
    "\n",
    "    # Process files in parallel\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(\n",
    "                process_csv_file,\n",
    "                csv_file,\n",
    "                adjacency2,\n",
    "                5733,      # test_iterations\n",
    "                0.001,     # sigma\n",
    "                threshold_tfce,\n",
    "                max_step,\n",
    "                t_power\n",
    "            ): csv_file for csv_file in csv_files[:file_limit]\n",
    "        }\n",
    "\n",
    "        # Collect results as they complete\n",
    "        for future in as_completed(futures):\n",
    "            csv_file_path, result = future.result()\n",
    "            results[csv_file_path] = result\n",
    "            print(f\"Completed processing {csv_file_path}\")\n",
    "\n",
    "        #     # Save results to CSV\n",
    "        # for csv_file_path, result in results.items():\n",
    "        #     # Convert results to a DataFrame\n",
    "        #     out = pd.DataFrame(result['out'])\n",
    "        #     out2 = pd.DataFrame(result['out2'])\n",
    "        #\n",
    "        #     # Calculate max and min for the second row of 'out'\n",
    "        #     second_row_out = out.iloc[1]\n",
    "        #     max_value_out = second_row_out.max()\n",
    "        #     min_value_out = second_row_out.min()\n",
    "        #\n",
    "        #     # Calculate max and min for the second row of 'out2'\n",
    "        #     second_row_out2 = out2.iloc[1]\n",
    "        #     max_value_out2 = second_row_out2.max()\n",
    "        #     min_value_out2 = second_row_out2.min()\n",
    "        #\n",
    "        #     print(f\"Max value in the second row of 'out': {max_value_out}\")\n",
    "        #     print(f\"Min value in the second row of 'out': {min_value_out}\")\n",
    "        #     print(f\"Max value in the second row of 'out2': {max_value_out2}\")\n",
    "        #     print(f\"Min value in the second row of 'out2': {min_value_out2}\")\n",
    "\n",
    "        # Initialize lists to store min and max values\n",
    "        max_values_out = []\n",
    "        min_values_out = []\n",
    "        max_values_out2 = []\n",
    "        min_values_out2 = []\n",
    "\n",
    "        # Collect results\n",
    "        for csv_file_path, result in results.items():\n",
    "\n",
    "            # Convert results to a DataFrame\n",
    "            # out structure\n",
    "            # out have two row, the second row is tfce values, so you just need to get the max and min of the second row.\n",
    "            out = pd.DataFrame(result['out'])\n",
    "            out2 = pd.DataFrame(result['out2'])\n",
    "\n",
    "            # Calculate max and min for the second row of 'out'\n",
    "            second_row_out = out.iloc[1]\n",
    "            max_values_out.append(second_row_out.max())\n",
    "            min_values_out.append(second_row_out.min())\n",
    "\n",
    "            # Calculate max and min for the second row of 'out2'\n",
    "            second_row_out2 = out2.iloc[1]\n",
    "            max_values_out2.append(second_row_out2.max())\n",
    "            min_values_out2.append(second_row_out2.min())\n",
    "\n",
    "            print(f\"Max value in the second row of 'out': {max_values_out[-1]}\")\n",
    "            print(f\"Min value in the second row of 'out': {min_values_out[-1]}\")\n",
    "            print(f\"Max value in the second row of 'out2': {max_values_out2[-1]}\")\n",
    "            print(f\"Min value in the second row of 'out2': {min_values_out2[-1]}\")\n",
    "\n",
    "        # # Save accumulated min and max values to CSV files\n",
    "        # pd.DataFrame(max_values_out, columns=['max_value_out']).to_csv(os.path.join(csv_dir, \"max_values_out.csv\"),\n",
    "        #                                                                index=False)\n",
    "        # pd.DataFrame(min_values_out, columns=['min_value_out']).to_csv(os.path.join(csv_dir, \"min_values_out.csv\"),\n",
    "        #                                                                index=False)\n",
    "        # pd.DataFrame(max_values_out2, columns=['max_value_out2']).to_csv(os.path.join(csv_dir, \"max_values_out2.csv\"),\n",
    "        #                                                                  index=False)\n",
    "        # pd.DataFrame(min_values_out2, columns=['min_value_out2']).to_csv(os.path.join(csv_dir, \"min_values_out2.csv\"),\n",
    "        #                                                                  index=False)\n",
    "\n",
    "\n",
    "        # # Save accumulated min and max values to CSV files with num_steps in the filename\n",
    "        # max_out_filename = f\"max_values_out_num_steps_{num_steps}.csv\"\n",
    "        # min_out_filename = f\"min_values_out_num_steps_{num_steps}.csv\"\n",
    "        # max_out2_filename = f\"max_values_out2_num_steps_{num_steps}.csv\"\n",
    "        # min_out2_filename = f\"min_values_out2_num_steps_{num_steps}.csv\"\n",
    "\n",
    "        # Get the current date in the desired format\n",
    "        current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "        # Define the filenames with date and file_limit information\n",
    "        max_out_filename = f\"max_values_out_{current_date}_num_steps_{num_steps}_file_limit_{file_limit}.csv\"\n",
    "        min_out_filename = f\"min_values_out_{current_date}_num_steps_{num_steps}_file_limit_{file_limit}.csv\"\n",
    "        max_out2_filename = f\"max_values_out2_{current_date}_num_steps_{num_steps}_file_limit_{file_limit}.csv\"\n",
    "        min_out2_filename = f\"min_values_out2_{current_date}_num_steps_{num_steps}_file_limit_{file_limit}.csv\"\n",
    "\n",
    "        pd.DataFrame(max_values_out, columns=['max_value_out']).to_csv(os.path.join(csv_dir, max_out_filename), index=False)\n",
    "        pd.DataFrame(min_values_out, columns=['min_value_out']).to_csv(os.path.join(csv_dir, min_out_filename), index=False)\n",
    "        pd.DataFrame(max_values_out2, columns=['max_value_out2']).to_csv(os.path.join(csv_dir, max_out2_filename), index=False)\n",
    "        pd.DataFrame(min_values_out2, columns=['min_value_out2']).to_csv(os.path.join(csv_dir, min_out2_filename), index=False)\n",
    "\n",
    "print(\"Done.\")\n",
    "print(results)\n"
   ],
   "id": "c0b0ea6bf564db0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reload stuff module\n",
      "D:\\ProgramData\\anaconda3\\envs\\mne12\\python.exe\n",
      "导入成功\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T23:50:43.963514800Z",
     "start_time": "2025-01-18T23:50:29.038553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "'''\n",
    "\n",
    "I don't know why \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "results = {}\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "# Reload custom module once more if needed\n",
    "reload(stuff)\n",
    "\n",
    "# Parameters\n",
    "max_step = 1\n",
    "t_power = 1\n",
    "n_tests = 59 * 91\n",
    "n_times = 91\n",
    "# file_limit = 3 # Number of files to process 490\n",
    "# max_workers = 1\n",
    "file_limit = len(csv_files)\n",
    "max_workers = 8\n",
    "\n",
    "num_steps = cluster_level_backup.num_steps\n",
    "\n",
    "# Process files in parallel\n",
    "with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = {\n",
    "        executor.submit(\n",
    "            process_csv_file,\n",
    "            csv_file,\n",
    "            adjacency2,\n",
    "            5733,      # test_iterations\n",
    "            0.001,     # sigma\n",
    "            threshold_tfce,\n",
    "            max_step,\n",
    "            t_power\n",
    "        ): csv_file for csv_file in csv_files[:file_limit]\n",
    "    }\n",
    "\n",
    "    # Collect results as they complete\n",
    "    for future in as_completed(futures):\n",
    "        csv_file_path, result = future.result()\n",
    "        results[csv_file_path] = result\n",
    "        print(f\"Completed processing {csv_file_path}\")\n",
    "\n",
    "    #     # Save results to CSV\n",
    "    # for csv_file_path, result in results.items():\n",
    "    #     # Convert results to a DataFrame\n",
    "    #     out = pd.DataFrame(result['out'])\n",
    "    #     out2 = pd.DataFrame(result['out2'])\n",
    "    #\n",
    "    #     # Calculate max and min for the second row of 'out'\n",
    "    #     second_row_out = out.iloc[1]\n",
    "    #     max_value_out = second_row_out.max()\n",
    "    #     min_value_out = second_row_out.min()\n",
    "    #\n",
    "    #     # Calculate max and min for the second row of 'out2'\n",
    "    #     second_row_out2 = out2.iloc[1]\n",
    "    #     max_value_out2 = second_row_out2.max()\n",
    "    #     min_value_out2 = second_row_out2.min()\n",
    "    #\n",
    "    #     print(f\"Max value in the second row of 'out': {max_value_out}\")\n",
    "    #     print(f\"Min value in the second row of 'out': {min_value_out}\")\n",
    "    #     print(f\"Max value in the second row of 'out2': {max_value_out2}\")\n",
    "    #     print(f\"Min value in the second row of 'out2': {min_value_out2}\")\n",
    "\n",
    "    # Initialize lists to store min and max values\n",
    "    max_values_out = []\n",
    "    min_values_out = []\n",
    "    max_values_out2 = []\n",
    "    min_values_out2 = []\n",
    "\n",
    "    # Collect results\n",
    "    for csv_file_path, result in results.items():\n",
    "\n",
    "        # Convert results to a DataFrame\n",
    "        # out structure\n",
    "        # out have two row, the second row is tfce values, so you just need to get the max and min of the second row.\n",
    "        out = pd.DataFrame(result['out'])\n",
    "        out2 = pd.DataFrame(result['out2'])\n",
    "\n",
    "        # Calculate max and min for the second row of 'out'\n",
    "        second_row_out = out.iloc[1]\n",
    "        max_values_out.append(second_row_out.max())\n",
    "        min_values_out.append(second_row_out.min())\n",
    "\n",
    "        # Calculate max and min for the second row of 'out2'\n",
    "        second_row_out2 = out2.iloc[1]\n",
    "        max_values_out2.append(second_row_out2.max())\n",
    "        min_values_out2.append(second_row_out2.min())\n",
    "\n",
    "        print(f\"Max value in the second row of 'out': {max_values_out[-1]}\")\n",
    "        print(f\"Min value in the second row of 'out': {min_values_out[-1]}\")\n",
    "        print(f\"Max value in the second row of 'out2': {max_values_out2[-1]}\")\n",
    "        print(f\"Min value in the second row of 'out2': {min_values_out2[-1]}\")\n",
    "\n",
    "    # # Save accumulated min and max values to CSV files\n",
    "    # pd.DataFrame(max_values_out, columns=['max_value_out']).to_csv(os.path.join(csv_dir, \"max_values_out.csv\"),\n",
    "    #                                                                index=False)\n",
    "    # pd.DataFrame(min_values_out, columns=['min_value_out']).to_csv(os.path.join(csv_dir, \"min_values_out.csv\"),\n",
    "    #                                                                index=False)\n",
    "    # pd.DataFrame(max_values_out2, columns=['max_value_out2']).to_csv(os.path.join(csv_dir, \"max_values_out2.csv\"),\n",
    "    #                                                                  index=False)\n",
    "    # pd.DataFrame(min_values_out2, columns=['min_value_out2']).to_csv(os.path.join(csv_dir, \"min_values_out2.csv\"),\n",
    "    #                                                                  index=False)\n",
    "\n",
    "\n",
    "    # # Save accumulated min and max values to CSV files with num_steps in the filename\n",
    "    # max_out_filename = f\"max_values_out_num_steps_{num_steps}.csv\"\n",
    "    # min_out_filename = f\"min_values_out_num_steps_{num_steps}.csv\"\n",
    "    # max_out2_filename = f\"max_values_out2_num_steps_{num_steps}.csv\"\n",
    "    # min_out2_filename = f\"min_values_out2_num_steps_{num_steps}.csv\"\n",
    "\n",
    "    # Get the current date in the desired format\n",
    "    current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "    # Define the filenames with date and file_limit information\n",
    "    max_out_filename = f\"max_values_out_{current_date}_num_steps_{num_steps}_file_limit_{file_limit}.csv\"\n",
    "    min_out_filename = f\"min_values_out_{current_date}_num_steps_{num_steps}_file_limit_{file_limit}.csv\"\n",
    "    max_out2_filename = f\"max_values_out2_{current_date}_num_steps_{num_steps}_file_limit_{file_limit}.csv\"\n",
    "    min_out2_filename = f\"min_values_out2_{current_date}_num_steps_{num_steps}_file_limit_{file_limit}.csv\"\n",
    "\n",
    "    pd.DataFrame(max_values_out, columns=['max_value_out']).to_csv(os.path.join(csv_dir, max_out_filename), index=False)\n",
    "    pd.DataFrame(min_values_out, columns=['min_value_out']).to_csv(os.path.join(csv_dir, min_out_filename), index=False)\n",
    "    pd.DataFrame(max_values_out2, columns=['max_value_out2']).to_csv(os.path.join(csv_dir, max_out2_filename), index=False)\n",
    "    pd.DataFrame(min_values_out2, columns=['min_value_out2']).to_csv(os.path.join(csv_dir, min_out2_filename), index=False)\n",
    "\n"
   ],
   "id": "8acb34fdcc19ed4b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reload stuff module\n",
      "D:\\ProgramData\\anaconda3\\envs\\mne12\\python.exe\n",
      "导入成功\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "18f1d65a071889d6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne12",
   "language": "python",
   "name": "mne12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
