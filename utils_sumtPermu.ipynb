{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c4e74fcd67a2467",
   "metadata": {},
   "source": [
    "find cluster for original data\n",
    "find cluster for permutation data\n",
    "\n",
    "输入是从R那里得到的时空点的t p csv文档。\n",
    "\n",
    "你算中介model的时候其实有两类\n",
    "1 每个点都算\n",
    "2 时间切片酸\n",
    "\n",
    "\n",
    "\n",
    "这段代码的函数有被重用的可能性，其实应该弄到 .py 文件里，调用来使用。\n",
    "\n",
    "这样你就不用每次跑到这个脚本里来跑代码，而是在单一的 脚本文件里 调用就行\n",
    "\n",
    "那直接把脚本转换成\n",
    "\n",
    "----\n",
    "\n",
    "why use p value rather than t value to find clusters?\n",
    "\n",
    "notice: you cannot use t-threshold to find clusters, because there is no constant t threshold for \n",
    "each temporal-spatial point.\n",
    "For each temporal-spatial point, the t-threshold is different.\n",
    "Because the degree of freedom is different for linear mixed model!\n",
    "\n",
    "So you read the rds file to load summary results, and extract the p value for each temporal-spatial point.\n",
    "we have to use the p value to find clusters.\n",
    "\n",
    "\n",
    "\n",
    "calculate the permutation sum-t\n",
    "\n",
    "prodedure:\n",
    "1 calculate sum-t\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "id": "9eaa18361f4af1a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T22:53:15.558794Z",
     "start_time": "2025-09-03T22:53:13.185285Z"
    }
   },
   "source": [
    "import os\n",
    "import glob\n",
    "import mne\n",
    "import mne.stats.cluster_level_backup as cluster_level_backup\n",
    "from importlib import reload\n",
    "import utils_EEG.stuff as stuff\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from mne.channels import find_ch_adjacency\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "# Reload custom module to ensure latest changes\n",
    "reload(stuff)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reload ok\n",
      "reload stuff module\n",
      "D:\\ProgramData\\anaconda3\\envs\\mne12\\python.exe\n",
      "导入成功\n",
      "reload stuff module\n",
      "D:\\ProgramData\\anaconda3\\envs\\mne12\\python.exe\n",
      "导入成功\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'utils_EEG.stuff' from 'D:\\\\LYW\\\\pre10\\\\utils_EEG\\\\stuff.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "20ddb0cf8e461bf8",
   "metadata": {},
   "source": [
    "## define function to find clusters using sum-t method from csv file with p-values\n",
    "\n",
    "用p还是用t，需要改两个地方\n",
    "1 除了设定 threshold_type 为 'p' 或 't' 外\n",
    "2 还要把 p_values 放到参数中\n",
    "\n",
    "    threshold_type= threshold_type, # 'p' or 't'\n",
    "\n",
    "    Extract the p-values column\n",
    "    p_values = data['p_value'].values # if you want to use p-values instead of t-values\n",
    "    p_values=p_values # if you want to use p-values instead of t-values\n",
    "        \n",
    "\n",
    "函数需要和调用在一个block中才能debug跳转。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d31adf347caa4499",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T22:53:26.846537Z",
     "start_time": "2025-09-03T22:53:26.734026Z"
    }
   },
   "source": [
    "# Modify the process_csv_file function to use sum-t clustering\n",
    "# include\n",
    "# find cluster \n",
    "def calculate_sumT_from_csv_file(\n",
    "        csv_file_path,\n",
    "        adjacency2, \n",
    "        test_iterations,\n",
    "        sigma, \n",
    "        threshold_sum_t, \n",
    "        max_step, \n",
    "        t_power, \n",
    "        threshold_type,\n",
    "        tail=0\n",
    "):\n",
    "\n",
    "    print(f\"Processing {csv_file_path}\")\n",
    "\n",
    "    '''\n",
    "    why don't you use cluster_level_backup.spatio_temporal_cluster_test() directly?\n",
    "    because you didn't add a new parameter to read csv file.\n",
    "    '''\n",
    "\n",
    "    # Compute t based on the csv file\n",
    "    # using coef_estimate and std_error from csv file\n",
    "    # and sigma to smooth the t values.\n",
    "    # I should change the function name to calculate_t_values_from_csv_file。。。\n",
    "    # smoothing..\n",
    "    # if you don't need smoothing, then set sigma = 0 or load from csv file directly\n",
    "    t_obs_orig_permu = stuff.apply_lmer_models_to_array_parallel_hat(\n",
    "        array_of_dfs=None,\n",
    "        test_iterations=test_iterations,#useless\n",
    "        max_workers=5,# useless,no parallel process in this version function.\n",
    "        sigma=sigma,\n",
    "        csv_file_path=csv_file_path # default csv file path: summary_results_2024年11月14日.csv\n",
    "    )\n",
    "\n",
    "    # load p-values from csv file\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Extract the p-values column\n",
    "    p_values = data['p_value'].values # if you want to use p-values instead of t-values\n",
    "\n",
    "    # Calculate max and min values of t_obs_orig_permu\n",
    "    max_t_obs = np.max(t_obs_orig_permu)\n",
    "    min_t_obs = np.min(t_obs_orig_permu)\n",
    "    print(f\"Max value of t_obs_orig_permu: {max_t_obs}\")\n",
    "    print(f\"Min value of t_obs_orig_permu: {min_t_obs}\")\n",
    "\n",
    "    # Find clusters using sum-t method\n",
    "    # I have to change the function to use p-values instead of t-values for the clustering.\n",
    "    out = cluster_level_backup._find_clusters_TorP(\n",
    "        x=t_obs_orig_permu,\n",
    "        threshold=threshold_sum_t,\n",
    "        tail=tail,\n",
    "        adjacency=adjacency2,\n",
    "        max_step=max_step,\n",
    "        include=None,\n",
    "        partitions=None,\n",
    "        t_power=t_power,\n",
    "        show_info=True,\n",
    "        # additional\n",
    "        threshold_type= threshold_type, # 'p' or 't'\n",
    "        p_values=p_values # if you want to use p-values instead of t-values\n",
    "    )\n",
    "    clustersT, clusterT_sum = out\n",
    "\n",
    "    return csv_file_path, {'out': out} # if you want all cluster information, you can return it.\n",
    "    # # Calculate min and max of cluster sums\n",
    "    # min_cluster_sum = np.min(clusterT_sum)\n",
    "    # max_cluster_sum = np.max(clusterT_sum)\n",
    "    #\n",
    "    # return csv_file_path, min_cluster_sum, max_cluster_sum\n",
    "\n",
    "\n",
    "# Define the set of channels to use (excluding FT9, FT10, TP9, TP10)\n",
    "channels_all = [\n",
    "    'Fp1', 'Fp2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4',\n",
    "    'O1', 'O2', 'F7', 'F8', 'T7', 'T8', 'P7', 'P8',\n",
    "    'Fz', 'Cz', 'Pz', 'FC1', 'FC2', 'CP1', 'CP2',\n",
    "    'FC5', 'FC6', 'CP5', 'CP6', 'F1', 'F2', 'C1',\n",
    "    'C2', 'P1', 'P2', 'AF3', 'AF4', 'FC3', 'FC4',\n",
    "    'CP3', 'CP4', 'PO3', 'PO4', 'F5', 'F6', 'C5',\n",
    "    'C6', 'P5', 'P6', 'AF7', 'AF8', 'FT7', 'FT8',\n",
    "    'TP7', 'TP8', 'PO7', 'PO8', 'Fpz', 'CPz', 'POz', 'Oz'\n",
    "]\n",
    "\n",
    "# Load an evoked file as an example\n",
    "file_path = r\"D:\\LYW\\pre10\\data\\7evoked_allWords\\prex006-ave.fif\"\n",
    "evoked_example = mne.read_evokeds(file_path, proj=False, verbose=None)\n",
    "evoked_example = evoked_example[0].resample(91)  # Resample to 91 Hz\n",
    "evoked_example.pick_channels(channels_all)  # Select a subset of channels\n",
    "\n",
    "# Compute adjacency for the chosen channels\n",
    "info = evoked_example.info\n",
    "adjacency, ch_names = find_ch_adjacency(info, ch_type=\"eeg\")\n",
    "\n",
    "# Set parameters related to cluster analysis\n",
    "max_step = 1\n",
    "t_power = 1\n",
    "n_tests = 59 * 91\n",
    "n_times = 91\n",
    "\n",
    "# Setup adjacency structure required for TFCE calculations\n",
    "adjacency2 = cluster_level_backup._setup_adjacency(adjacency, n_tests, n_times)\n",
    "\n",
    "test_iterations = 5733 # useless\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading D:\\LYW\\pre10\\data\\7evoked_allWords\\prex006-ave.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     800.00 ms (0.40 × 100 + 0.20 × 101 + 0.20 × 102 + 0.20 × 103)\n",
      "        0 CTF compensation matrices available\n",
      "        nave = 250 - aspect type = 100\n",
      "Loaded Evoked data is baseline-corrected (baseline: [-0.2, 0] s)\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Could not find a adjacency matrix for the data. Computing adjacency based on Delaunay triangulations.\n",
      "-- number of adjacent vertices : 59\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "4b0abfdf8f722e05",
   "metadata": {},
   "source": [
    "## 找cluster \n",
    "\n",
    "bug: 为何算出来的sum_T 都是inf?\n",
    "\n",
    "用t或者用p要改两个地方：\n",
    "threshold_sum_t = 0.01 # if you set threshold_type='p',\n",
    "threshold_sum_t = 1.96 # if you set threshold_type='t',\n",
    "    'p' # if you set threshold_type='p',\n",
    "    't' # if you set threshold_type='t',\n",
    "\n",
    "\n",
    "无论是对多个permutation的结果找cluster，还是对原始数据结果找cluster，都调用一样的函数的。\n",
    "\n",
    "而t值根据各种情况不同而不同。\n",
    "1 一开始的时候是根据mix effect model算的t，亮哥教你的算法。\n",
    "2 你后来做的用mplus，各种estimator算的，中介模型。\n",
    "    a b ab\n",
    "\n",
    "\n",
    "\n",
    "D:\\LYW\\pre10\\utils_EEG\\拿到a b ab的z p.ipynb\n",
    "算出来：\n",
    "output_file = f\"D:\\\\LYW\\\\pre10\\\\data\\\\permutation_summary_between\\\\summary_results_2025年5月28日_ab_updated.csv\""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### d",
   "id": "5f0682762bfd98b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T07:00:35.049038Z",
     "start_time": "2025-08-13T07:00:34.782991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "'''\n",
    "D:\\\\LYW\\R for permutation t\\\\Mplus\\\\results\\\\all_spatiotemporal_results_WLSMV_precise_parallel.csv\n",
    "这个文件地址算的结果包含了 a b c ab 的精确 estimate 和  std_error\n",
    "这样就可以判断 estimate的正负，函数才能起作用。\n",
    "\n",
    "直接把 ab_estimate_dat\tab_std_error_dat改成 coef_estimate std_error 就行了，然后相除得到另一行 t_value\n",
    "\n",
    "\n",
    "需要注意，你关心的是 a为负，但是间接效应ab是正的。\n",
    "所以 对于ab,你需要设置 tail=1，t = 1.96\n",
    "\n",
    "ab>0,然后要分两类\n",
    "a<0  分散学习带来更多变异，那么因该出现在中晚期，促进逻辑分上升。\n",
    "a>0  集中学习带来更多变异，出现在早期，这种变异遏制逻辑分上升。\n",
    "一开始的方向有正负 后面对应的是 促进 和 遏制，负负得正 才是一对变化。\n",
    "\n",
    "那还得再改一次Excel表格\n",
    "'''\n",
    "\n",
    "'''\n",
    "\n",
    "需要对原始的 csv文件做一些预处理\n",
    "\n",
    "1 允许我去选择 a b ab，增加四列coef_estimate std_erorr ，t_value p_value, 就根据其中对应的两列数据来得到！\n",
    "2  如果是ab，那么需要通过a 大于0 小于0 来分成两个文档。\n",
    "\n",
    "\n",
    "'''\n",
    "def add_standard_stats_columns(csv_file_path, effect_type='ab',):\n",
    "      \"\"\"\n",
    "      Add standard statistical columns (coef_estimate, std_error,\n",
    "  t_value, p_value)\n",
    "      based on selected effect type (a, b, or ab).\n",
    "\n",
    "      Parameters:\n",
    "      - csv_file_path: str, path to the CSV file containing mediation\n",
    "  results\n",
    "      - effect_type: str, 'a', 'b', or 'ab' to select which effect to\n",
    "  analyze\n",
    "      - output_path: str, optional path to save the modified CSV. If\n",
    "  None, returns DataFrame only.\n",
    "\n",
    "      Returns:\n",
    "      - pandas.DataFrame with added columns\n",
    "      \"\"\"\n",
    "      import pandas as pd\n",
    "      import numpy as np\n",
    "      from scipy import stats\n",
    "\n",
    "      # Load data\n",
    "      data = pd.read_csv(csv_file_path)\n",
    "\n",
    "      # Define column mappings based on effect type\n",
    "      column_mapping = {\n",
    "          'a': {'estimate': 'a_estimate_dat', 'std_error':\n",
    "  'a_std_error_dat'},\n",
    "          'b': {'estimate': 'b_estimate_dat', 'std_error':\n",
    "  'b_std_error_dat'},\n",
    "          'ab': {'estimate': 'ab_estimate_dat', 'std_error':\n",
    "  'ab_std_error_dat'}\n",
    "      }\n",
    "\n",
    "      if effect_type not in column_mapping:\n",
    "          raise ValueError(\"effect_type must be 'a', 'b', or 'ab'\")\n",
    "\n",
    "      # Get the appropriate columns\n",
    "      estimate_col = column_mapping[effect_type]['estimate']\n",
    "      std_error_col = column_mapping[effect_type]['std_error']\n",
    "\n",
    "      # Add standard columns\n",
    "      data['coef_estimate'] = data[estimate_col]\n",
    "      data['std_error'] = data[std_error_col]\n",
    "\n",
    "      # Calculate t-values (avoid division by zero)\n",
    "      with np.errstate(divide='ignore', invalid='ignore'):\n",
    "          data['t_value'] = data['coef_estimate'] / data['std_error']\n",
    "          data['t_value'] = data['t_value'].replace([np.inf, -np.inf],\n",
    "  np.nan)\n",
    "\n",
    "      # Calculate p-values (two-tailed test)\n",
    "      data['p_value'] = 2 * (1 -\n",
    "  stats.norm.cdf(np.abs(data['t_value'])))\n",
    "      data['p_value'] = data['p_value'].fillna(1.0)  # NaN t-values get p=1\n",
    "\n",
    "      print(f\"Added standard columns based on {effect_type} effect:\")\n",
    "      print(f\"  coef_estimate <- {estimate_col}\")\n",
    "      print(f\"  std_error <- {std_error_col}\")\n",
    "      print(f\"  t_value = coef_estimate / std_error\")\n",
    "      print(f\"  p_value = 2 * (1 - norm.cdf(|t_value|))\")\n",
    "\n",
    "    # Generate output path by adding effect_type to original filename\n",
    "      base_name, ext = os.path.splitext(csv_file_path)\n",
    "      output_path = f\"{base_name}_{effect_type}{ext}\"\n",
    "\n",
    "      # Save the modified data\n",
    "      data.to_csv(output_path, index=False)\n",
    "      print(f\"Saved to: {output_path}\")\n",
    "\n",
    "      return data\n",
    "\n",
    " # 增加_dat 高精度数据\n",
    "csv_path = r\"D:\\LYW\\R for permutation t\\Mplus\\results\\all_spatiotemporal_results_WLSMV_precise_parallel.csv\"\n",
    "df_ab = add_standard_stats_columns(csv_path, effect_type='ab')\n",
    "df_a = add_standard_stats_columns(csv_path, effect_type='a')\n",
    "df_b = add_standard_stats_columns( csv_path, effect_type='b')"
   ],
   "id": "823f54bbbce834a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added standard columns based on ab effect:\n",
      "  coef_estimate <- ab_estimate_dat\n",
      "  std_error <- ab_std_error_dat\n",
      "  t_value = coef_estimate / std_error\n",
      "  p_value = 2 * (1 - norm.cdf(|t_value|))\n",
      "Saved to: D:\\LYW\\R for permutation t\\Mplus\\results\\all_spatiotemporal_results_WLSMV_precise_parallel_ab.csv\n",
      "Added standard columns based on a effect:\n",
      "  coef_estimate <- a_estimate_dat\n",
      "  std_error <- a_std_error_dat\n",
      "  t_value = coef_estimate / std_error\n",
      "  p_value = 2 * (1 - norm.cdf(|t_value|))\n",
      "Saved to: D:\\LYW\\R for permutation t\\Mplus\\results\\all_spatiotemporal_results_WLSMV_precise_parallel_a.csv\n",
      "Added standard columns based on b effect:\n",
      "  coef_estimate <- b_estimate_dat\n",
      "  std_error <- b_std_error_dat\n",
      "  t_value = coef_estimate / std_error\n",
      "  p_value = 2 * (1 - norm.cdf(|t_value|))\n",
      "Saved to: D:\\LYW\\R for permutation t\\Mplus\\results\\all_spatiotemporal_results_WLSMV_precise_parallel_b.csv\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T04:38:14.529790Z",
     "start_time": "2025-09-04T04:38:14.369992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "\n",
    "如果是ab，那么需要通过a 大于0 小于0 来分成两个文档。\n",
    "\n",
    "我现在需要把 \"D:\\LYW\\R for permutation t\\Mplus\\results\\all_spatiotemporal_results_WLSMV_precise_parallel_ab.csv\" 这个文件变成两个。\n",
    "其中一个，要求检查每一行 a_estimate_dat，如果大于0，那么t_value 被赋值为0 ，保存all_spatiotemporal_results_WLSMV_precise_parallel_ab_a大于0\n",
    "另一个反过来。\n",
    "\n",
    "\n",
    "这个脚本会：\n",
    "\n",
    "  1. 读取原始CSV文件\n",
    "  2. 添加标准统计列 (基于ab效应)\n",
    "  3. 创建两个副本：\n",
    "    - all_spatiotemporal_results_WLSMV_precise_parallel_ab_a小于等于0.csv:\n",
    "  当 a_estimate_dat > 0 时，将 t_value 设为0 (保留a<=0的效应)\n",
    "    -\n",
    "  all_spatiotemporal_results_WLSMV_precise_parallel_ab_a大于0.csv:\n",
    "  当 a_estimate_dat <= 0 时，将 t_value 设为0 (保留a>0的效应)\n",
    "  4. 同时调整p值: 当t_value被设为0时，p_value也设为1.0\n",
    "\n",
    "----\n",
    "\n",
    "a 和 b 就不需要split了。\n",
    "\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import os\n",
    "\n",
    "def split_csv_by_a_estimate(csv_file_path):\n",
    "  \"\"\"\n",
    "  Split CSV file into two based on a_estimate_dat values, with\n",
    "conditional t_value assignment.\n",
    "\n",
    "  Parameters:\n",
    "  - csv_file_path: str, path to the original CSV file\n",
    "  \"\"\"\n",
    "\n",
    "  # Load data\n",
    "  data = pd.read_csv(csv_file_path)\n",
    "\n",
    "  # Add standard statistical columns for ab effect\n",
    "  data['coef_estimate'] = data['ab_estimate_dat']\n",
    "  data['std_error'] = data['ab_std_error_dat']\n",
    "\n",
    "  # Calculate t-values\n",
    "  with np.errstate(divide='ignore', invalid='ignore'):\n",
    "      data['t_value'] = data['coef_estimate'] / data['std_error']\n",
    "      data['t_value'] = data['t_value'].replace([np.inf, -np.inf],\n",
    "np.nan)\n",
    "\n",
    "  # Calculate p-values\n",
    "  data['p_value'] = 2 * (1 -\n",
    "stats.norm.cdf(np.abs(data['t_value'])))\n",
    "  data['p_value'] = data['p_value'].fillna(1.0)\n",
    "\n",
    "  # Create two copies of the data\n",
    "  data_when_positive_zero = data.copy()  # a>0时t_value=0，保留a<=0的效应\n",
    "  data_when_negative_zero = data.copy()  # a<=0时t_value=0，保留a>0的效应\n",
    "\n",
    "  # When a_estimate > 0, set t_value to 0 (keeping only negative a effects)\n",
    "  mask_positive = data_when_positive_zero['a_estimate_dat'] >= 0\n",
    "  data_when_positive_zero.loc[mask_positive, 't_value'] = 0\n",
    "  data_when_positive_zero.loc[mask_positive, 'coef_estimate'] = 0  # 同时将coef_estimate设为0\n",
    "  data_when_positive_zero.loc[mask_positive, 'p_value'] = 1.0  # p-value = 1   when t = 0\n",
    "\n",
    "  # When a_estimate <= 0, set t_value to 0 (keeping only positive a effects)\n",
    "  mask_negative = data_when_negative_zero['a_estimate_dat'] <= 0\n",
    "  data_when_negative_zero.loc[mask_negative, 't_value'] = 0\n",
    "  data_when_negative_zero.loc[mask_negative, 'coef_estimate'] = 0  # 同时将coef_estimate设为0\n",
    "  data_when_negative_zero.loc[mask_negative, 'p_value'] = 1.0  # p-value = 1     when t = 0\n",
    "\n",
    "  # Generate output paths with corrected names\n",
    "  base_name, ext = os.path.splitext(csv_file_path)\n",
    "  output_path_keep_negative = f\"{base_name}_a小于等于0{ext}\"  # a>0时被置零，保留a<=0\n",
    "  output_path_keep_positive = f\"{base_name}_a大于0{ext}\"     # a<=0时被置零，保留a>0\n",
    "\n",
    "  # Save the files\n",
    "  data_when_positive_zero.to_csv(output_path_keep_negative, index=False)\n",
    "  data_when_negative_zero.to_csv(output_path_keep_positive, index=False)\n",
    "\n",
    "  print(f\"文件已分割:\")\n",
    "  print(f\"保留 a_estimate <= 0 的效应: {output_path_keep_negative}\")\n",
    "  print(f\"  - 有 {mask_positive.sum()} 行 (a>0) 被设为 t_value=0\")\n",
    "  print(f\"保留 a_estimate > 0 的效应: {output_path_keep_positive}\")\n",
    "  print(f\"  - 有 {mask_negative.sum()} 行 (a<=0) 被设为 t_value=0\")\n",
    "\n",
    "  return data_when_positive_zero, data_when_negative_zero\n",
    "\n",
    "# 使用示例\n",
    "csv_path = r\"D:\\LYW\\R for permutation t\\Mplus\\results\\all_spatiotemporal_results_WLSMV_precise_parallel_ab.csv\"\n",
    "df_keep_neg, df_keep_pos = split_csv_by_a_estimate(csv_path)"
   ],
   "id": "16a7f504083ba4d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件已分割:\n",
      "保留 a_estimate <= 0 的效应: D:\\LYW\\R for permutation t\\Mplus\\results\\all_spatiotemporal_results_WLSMV_precise_parallel_ab_a小于等于0.csv\n",
      "  - 有 1988 行 (a>0) 被设为 t_value=0\n",
      "保留 a_estimate > 0 的效应: D:\\LYW\\R for permutation t\\Mplus\\results\\all_spatiotemporal_results_WLSMV_precise_parallel_ab_a大于0.csv\n",
      "  - 有 3381 行 (a<=0) 被设为 t_value=0\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 算",
   "id": "926443eb8c981226"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T04:46:08.804832Z",
     "start_time": "2025-09-04T04:46:08.789323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 这两个是ab的\n",
    "csv_files = [\"D:\\\\LYW\\R for permutation t\\\\Mplus\\\\results\\\\all_spatiotemporal_results_WLSMV_precise_parallel_ab_a小于等于0.csv\"]\n",
    "# csv_files = [\"D:\\\\LYW\\R for permutation t\\\\Mplus\\\\results\\\\all_spatiotemporal_results_WLSMV_precise_parallel_ab_a大于0.csv\"]\n",
    "\n",
    "# # a 只要一个就行了，修改对应的 t  和 tail 的正负号就行了。\n",
    "# csv_files = [\"D:\\\\LYW\\R for permutation t\\\\Mplus\\\\results\\\\all_spatiotemporal_results_WLSMV_precise_parallel_a.csv\"]\n",
    "#\n",
    "# # b 也一样吧\n",
    "# csv_files = [\"D:\\\\LYW\\R for permutation t\\\\Mplus\\\\results\\\\all_spatiotemporal_results_WLSMV_precise_parallel_b.csv\"]\n",
    "\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "# original value file\n",
    "# summary_results_2024年11月14日_upaded.csv 这个文件是从D:/LYW/R for permutation t/loadRDS.qmd 中复制出来的，\n",
    "# 这个是mix effect model 算的结果，其实相当于后面算的 a_path\n",
    "# csv_files = [\"D:\\LYW\\pre10\\data\\permutation_summary_between\\summary_results_2024年11月14日_upaded.csv\"]\n",
    "# 这个是后面用中介模型算的结果， ab代表的是ab_path 算出来的显著点。\n",
    "# 筛选条件：ab显著为正，a显著为负，b显著为负\n",
    "# 问题：哪里的代码进行了筛选，难道是你之前手动筛选的？\n",
    "# 2025年7月22日到这个日期为止你是自己进行了筛选，把不符合要求的p改成了1 ，实际上是不对的？\n",
    "# csv_files = [\"D:\\\\LYW\\\\pre10\\\\data\\\\permutation_summary_between\\\\summary_results_2025年05月28日_ab_updated.csv\"]\n",
    "# 中介 - a\n",
    "# csv_files = [\"D:\\\\LYW\\\\pre10\\\\data\\\\permutation_summary_between\\\\summary_results_2025年05月29日_a_updated.csv\"]\n",
    "#\n",
    "# # 混合效应模型,这个适用的model formula <- 'distance ~ category_cond2 + logicalScore1Diff + RT1Diff + (1|subId_cond1) ' 差值。\n",
    "# # 这里没有p值，因为你的R代码注释掉了生成RDS文件。\n",
    "# csv_files = [\"D:\\\\LYW\\\\pre10\\\\data\\8 temporal-spatial\\\\sub28\\\\summary_coef_std_20250704_133507.csv\"]\n"
   ],
   "id": "c2d1809e6685e03c",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T04:46:10.556091Z",
     "start_time": "2025-09-04T04:46:10.515559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 这个csv文件可以从别的地方来。\n",
    "# \n",
    "# Define a fixed threshold for sum-t clustering\n",
    " # Example threshold value, not like tfce, we have to set it manually.\n",
    " # not threshold free!\n",
    "# threshold_sum_t = 0.05 # if you set threshold_type='p',\n",
    "\n",
    "# 正负也要和tail一起变化?\n",
    "threshold_sum_t = 1.96 # if you set threshold_type='t', 1.96对应的是0.05, 2.576对应0.01,0.001 对应3.291\n",
    "\n",
    "# Tail parameter (set here to 1, meaning similarity massed < spaced, dissimilarity spaced < massed)\n",
    "# 这个tail是全局变量，所以是有用的。\n",
    "# 如果算的是ab，那么就肯定是正\n",
    "tail = 1\n",
    "\n",
    "# sigma = 0.001  # Set the smoothing parameter\n",
    "sigma = 0.01  # Set the smoothing parameter\n",
    "# sigma = 0 # I don't need smoothing for sum-t clustering\n",
    "\n",
    "# For debugging\n",
    "print(f\"adjacency2: {type(adjacency2)}\")\n",
    "print(f\"n_tests: {type(n_tests)} - {n_tests}\")\n",
    "print(f\"n_times: {type(n_times)} - {n_times}\")\n",
    "print(f\"threshold_tfce: {type(threshold_sum_t)} - {threshold_sum_t}\")\n",
    "\n",
    "# one shot test\n",
    "#\n",
    "csv_file_path, out = calculate_sumT_from_csv_file(\n",
    "    csv_files[0],\n",
    "    adjacency2,\n",
    "    test_iterations,\n",
    "    sigma,\n",
    "    threshold_sum_t,\n",
    "    max_step,\n",
    "    t_power,\n",
    "    # 'p' ,# if you set threshold_type='p',\n",
    "    't' ,# if you set threshold_type='t',\n",
    "    tail = tail\n",
    ")\n",
    "# Assuming out is a tuple (clustersT, clusterT_sum)\n",
    "clustersT, clusterT_sum = out['out']\n",
    "\n",
    "'''\n",
    "storing as csv file cannot keep the structure of the data.\n",
    "'''\n",
    "# # Convert to DataFrame\n",
    "# df = pd.DataFrame({\n",
    "#     'Cluster Indices': clustersT,\n",
    "#     'Cluster Sums': clusterT_sum\n",
    "# })\n",
    "# \n",
    "# # Get the current date\n",
    "current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "# \n",
    "# # Specify the file path with date and threshold information\n",
    "# output_csv_path = f\"D:\\\\LYW\\\\pre10\\\\data\\\\permutation_summary_between\\\\summary_results_{current_date}_thresh_{threshold_sum_t}_clustersT.csv\"\n",
    "# \n",
    "# \n",
    "# # Write the DataFrame to a CSV file\n",
    "# df.to_csv(output_csv_path, index=False)\n",
    "# \n",
    "# print(output_csv_path)\n",
    "\n",
    "import pickle\n",
    "# \n",
    "# # Store clustersT using pickle\n",
    "# pickle_path = f\"D:\\\\LYW\\\\pre10\\\\data\\\\permutation_summary_between\\\\clustersT_{current_date}_thresh_{threshold_sum_t}.pkl\"\n",
    "# with open(pickle_path, 'wb') as f:\n",
    "#     pickle.dump(clustersT, f)\n",
    "# \n",
    "# print(f\"Clusters stored at: {pickle_path}\")\n",
    "# \n",
    "# # Reload clustersT from the pickle file\n",
    "# with open(pickle_path, 'rb') as f:\n",
    "#     clustersT = pickle.load(f)\n",
    "# \n",
    "# # Verify the data\n",
    "# print(clustersT)\n",
    "# \n",
    "\n",
    "# 自动从csv文件名中提取信息来构建pickle文件名\n",
    "csv_filename = os.path.basename(csv_files[0])  # 获取文件名，例如: all_spatiotemporal_results_WLSMV_precise_parallel_ab_a大于0.csv\n",
    "csv_name_without_ext = os.path.splitext(csv_filename)[0]  # 去掉.csv扩展名\n",
    "\n",
    "# Store clustersT and clusterT_sum using pickle with auto-generated name\n",
    "pickle_path = f\"D:\\\\LYW\\\\pre10\\\\data\\\\permutation_summary_between\\\\clusters_{current_date}_thresh_{threshold_sum_t}_direction_{tail}_{csv_name_without_ext}.pkl\"\n",
    "with open(pickle_path, 'wb') as f:\n",
    "    pickle.dump((clustersT, clusterT_sum), f)\n",
    "\n",
    "print(f\"Clusters stored at: {pickle_path}\")\n",
    "\n",
    "# Reload clustersT and clusterT_sum from the pickle file\n",
    "with open(pickle_path, 'rb') as f:\n",
    "    clustersT2, clusterT_sum2 = pickle.load(f)\n",
    "\n",
    "# Verify the data\n",
    "print(clustersT2)\n",
    "print(clusterT_sum2)\n",
    "\n",
    "\n",
    "# 注意看找到的cluster保存在哪个文件里了Clusters stored at: D:\\LYW\\pre10\\data\\permutation_summary_between\\clusters_20250704_thresh_0.01.pkl\n",
    "# 到这里可以回到D:\\LYW\\pre10\\analysis\\nps\\nps_dist_mix_sep_epoch_searchlight_2025年1月2日.ipynb ： find clusters - sum_t version - considering different dgree of freedom"
   ],
   "id": "fc2274c91f2cbb87",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjacency2: <class 'list'>\n",
      "n_tests: <class 'int'> - 5369\n",
      "n_times: <class 'int'> - 91\n",
      "threshold_tfce: <class 'float'> - 1.96\n",
      "Processing D:\\LYW\\R for permutation t\\Mplus\\results\\all_spatiotemporal_results_WLSMV_precise_parallel_ab_a小于等于0.csv\n",
      "***Loading t values from CSV***, set globally before apply_lmer_models_to_array_parallel_hat\n",
      "D:\\LYW\\R for permutation t\\Mplus\\results\\all_spatiotemporal_results_WLSMV_precise_parallel_ab_a小于等于0.csv\n",
      "Max std_error (ignoring NaNs): 0.00049637841\n",
      "Results saved to results_t_original.npy and results_t_smooth.npy\n",
      "Max value of t_obs_orig_permu: 10.361620506609512\n",
      "Min value of t_obs_orig_permu: -6.6462751355121465\n",
      "Clusters stored at: D:\\LYW\\pre10\\data\\permutation_summary_between\\clusters_20250904_thresh_1.96_direction_1_all_spatiotemporal_results_WLSMV_precise_parallel_ab_a小于等于0.pkl\n",
      "[array([1337, 1365, 1396, 1410, 1414, 1424, 1455, 1469, 1430, 1461, 1483,\n",
      "       1514, 1528, 1489, 1520, 1526, 1362, 1421, 1446, 1480, 1505, 1492,\n",
      "       1539, 1564, 1578, 1551, 1590, 1552, 1565, 1566, 1540, 1573, 1579,\n",
      "       1587, 1542, 1548, 1559, 1585, 1598, 1623, 1637, 1610, 1649, 1611,\n",
      "       1599, 1638, 1607, 1618, 1644, 1657, 1682, 1696, 1669, 1674, 1665,\n",
      "       1681, 1708, 1684, 1670, 1683, 1658, 1664, 1703, 1666, 1677, 1716,\n",
      "       1741, 1728, 1733, 1732, 1740, 1767, 1743, 1742, 1729, 1723, 1754,\n",
      "       1762, 1782, 1813, 1787, 1791, 1792, 1799, 1800, 1826, 1802, 1788,\n",
      "       1841, 1846, 1850, 1851, 1885, 1861, 1847, 1894, 1919, 1941, 1906,\n",
      "       1944, 1920, 1905, 1900, 1953, 1978, 1986, 2000, 1965, 2003, 1955,\n",
      "       1961, 1979, 1964, 1959, 1893, 1952, 2011, 2036, 2023, 2035, 2062,\n",
      "       2024, 2037, 2038, 2012, 2045, 2051, 2059, 2014, 2020, 2070, 2095,\n",
      "       2082, 2084, 2094, 2121, 2083, 2096, 2097, 2071, 2104, 2110, 2118,\n",
      "       2073, 2079, 2090, 2123, 2021, 2080, 2119, 2129, 2154, 2160, 2141,\n",
      "       2143, 2153, 2180, 2151, 2142, 2155, 2156, 2181, 2130, 2163, 2164,\n",
      "       2132, 2133, 2182, 2169, 2177, 2178, 2138, 2149, 2139, 2175, 2167,\n",
      "       2188, 2213, 2219, 2200, 2203, 2199, 2202, 2212, 2191, 2222, 2236,\n",
      "       2240, 2241, 2192, 2201, 2237, 2198, 2208, 2226, 2234, 2247, 2272,\n",
      "       2278, 2262, 2258, 2261, 2271, 2250, 2281, 2295, 2285, 2293, 2306,\n",
      "       2309, 2354, 2317, 2320, 2321, 2328, 2330, 2379], dtype=int64), array([1901], dtype=int64), array([2494, 2538, 2571, 2585, 2592, 2553, 2565, 2556, 2564, 2596, 2643,\n",
      "       2651, 2597, 2612, 2630, 2644, 2615, 2623, 2624, 2655, 2702, 2710,\n",
      "       2656, 2671, 2689, 2703, 2675, 2682, 2683, 2715, 2762, 2761],\n",
      "      dtype=int64), array([2546, 2605], dtype=int64), array([2669, 2728, 2787], dtype=int64), array([2721, 2736, 2746, 2752, 2768, 2780, 2795, 2805, 2811, 2827, 2839,\n",
      "       2854, 2870, 2898, 2913, 2923, 2929, 2957, 2972, 2988, 3016, 3031,\n",
      "       3047, 3106], dtype=int64), array([2912, 2971], dtype=int64), array([3510, 3569, 3628, 3687, 3746, 3805, 3854, 3864, 3923, 3982, 4041,\n",
      "       4088, 4100, 4147], dtype=int64), array([3596, 3655, 3714, 3773, 3798, 3832, 3857, 3891], dtype=int64), array([3611], dtype=int64), array([3789, 3848, 3802, 3861, 3907, 3938, 3920, 3966, 3997, 4025, 4056,\n",
      "       4084, 4115, 4121, 4143], dtype=int64), array([4265, 4324], dtype=int64), array([4348, 4376, 4407, 4435, 4466, 4525, 4584, 4643, 4635, 4694, 4708,\n",
      "       4702, 4408, 4416, 4436, 4449, 4467, 4473, 4475, 4495, 4508, 4526,\n",
      "       4532, 4534, 4554, 4567, 4585, 4591, 4593, 4577, 4556, 4605, 4636,\n",
      "       4638, 4644, 4650, 4626, 4613, 4652, 4615, 4662, 4695, 4709, 4664,\n",
      "       4672, 4703, 4697, 4685, 4711, 4705, 4674, 4564, 4623, 4682, 4683,\n",
      "       4693, 4720, 4753, 4775, 4747, 4721, 4754, 4739, 4768, 4749, 4731,\n",
      "       4762, 4741, 4744, 4770, 4756, 4764, 4733, 4750, 4746, 4772, 4742,\n",
      "       4752, 4738, 4761, 4782, 4813, 4815, 4821, 4827, 4803, 4809, 4790,\n",
      "       4823, 4829, 4801, 4792, 4805, 4831, 4811, 4825, 4797, 4798, 4806,\n",
      "       4808, 4812, 4800, 4826, 4820, 4849, 4862, 4880, 4888, 4882, 4851,\n",
      "       4864, 4890, 4857, 4865, 4867, 4859, 4872, 4910, 4947, 4921, 4939,\n",
      "       4918, 4931, 4980, 4998, 5006], dtype=int64), array([4396, 4455], dtype=int64), array([4529, 4588, 4627, 4647, 4653, 4686, 4706, 4712, 4745, 4804, 4863,\n",
      "       4922, 4981], dtype=int64), array([4641], dtype=int64), array([5099, 5158, 5217], dtype=int64)]\n",
      "[798.98806781   2.06785663  81.64302478   5.22999242   7.60961711\n",
      "  80.55961722   4.41477447  33.68674362  20.30637471   2.17498995\n",
      "  38.62506344   4.75238442 341.78833995   4.25146247  42.28428461\n",
      "   2.07339234   7.12667547]\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "37f6bc9c4dbe38c8",
   "metadata": {},
   "source": [
    "## calculate the min and max cluster sums for each permutation - only save the max cluster sums\n",
    "\n",
    "和算原始数据的cluster不一样，并不是保存所有的cluster，每个permutation，只需要保存最大的那个就行。"
   ]
  },
  {
   "cell_type": "code",
   "id": "6ad686db26cd710b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T02:02:15.701137Z",
     "start_time": "2025-08-13T02:02:15.680743Z"
    }
   },
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Directory containing the CSV files for permutation summary\n",
    "csv_dir = r\"H:\\\\lyw\\\\sub28\"\n",
    "csv_files = glob.glob(os.path.join(csv_dir, \"summary_results_permutation_[0-9]*.csv\"))\n",
    "# Filter out anything that contains \"_updated\"\n",
    "csv_files = [f for f in csv_files \n",
    "             if \"_updated\" not in os.path.basename(f) \n",
    "             and \"updated_clustersT\" not in os.path.basename(f)]"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "3e524941033f0749",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "'''\n",
    "p-value\tOne-tailed Z-value\tTwo-tailed Z-value\n",
    "0.05\t1.645\t±1.96\n",
    "0.01\t2.33\t±2.58\n",
    "'''\n",
    "\n",
    "threshold_sum_t = 0.01 # Example threshold for p-value\n",
    "threshold_sum_t = 2.58 # Example threshold for t-value\n",
    "tail = 0\n",
    "sigma = 0.001\n",
    "max_step = 1  # Example value, adjust as needed\n",
    "t_power = 1  # Example value, adjust as needed\n",
    "\n",
    "# List to store results for each file\n",
    "cluster_sums_stats = []\n",
    "\n",
    "file_limit = len(csv_files)  # Limit the number of files to process\n",
    "# file_limit = 2\n",
    "# Process each CSV file\n",
    "for i, csv_file in enumerate(csv_files[:file_limit]):\n",
    "\n",
    "    # Perform the calculation\n",
    "    out = calculate_sumT_from_csv_file(\n",
    "        csv_file,\n",
    "        adjacency2,\n",
    "        test_iterations,\n",
    "        sigma,\n",
    "        threshold_sum_t,\n",
    "        max_step,\n",
    "        t_power,\n",
    "        threshold_type='t'\n",
    "    )\n",
    "    \n",
    "    # Assuming out is a tuple (clustersT, clusterT_sum)\n",
    "    clustersT, clusterT_sum = out\n",
    "    \n",
    "    # Calculate min and max from the second element of clusterT_sum['out'] \n",
    "    # which is the NumPy array\n",
    "    # min_sum = clusterT_sum['out'][1].min()\n",
    "    # max_sum = clusterT_sum['out'][1].max()\n",
    "    # Check if 'out' key exists and has enough elements\n",
    "    if 'out' in clusterT_sum and len(clusterT_sum['out']) > 1:\n",
    "        cluster_array = clusterT_sum['out'][1]\n",
    "        if cluster_array.size > 0:\n",
    "            min_sum = cluster_array.min()\n",
    "            max_sum = cluster_array.max()\n",
    "        else:\n",
    "            min_sum = None\n",
    "            max_sum = None\n",
    "    else:\n",
    "        min_sum = None\n",
    "        max_sum = None\n",
    "    \n",
    "    print(f\"For {csv_file}: min cluster sum = {min_sum}, max cluster sum = {max_sum}\")\n",
    "\n",
    "    # Append the results as a dictionary\n",
    "    cluster_sums_stats.append({\n",
    "        'File': csv_file,\n",
    "        'MinClusterSum': min_sum,\n",
    "        'MaxClusterSum': max_sum\n",
    "    })\n",
    "    \n",
    "    print(f\"Processed {csv_file} and saved stats.\")\n",
    "\n",
    "# Convert the list of dicts to a DataFrame\n",
    "cluster_sums_df = pd.DataFrame(cluster_sums_stats)\n",
    "\n",
    "# Get the current date\n",
    "current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "# Specify the file path with date and threshold information\n",
    "output_csv_path = os.path.join(\n",
    "    csv_dir,\n",
    "    f\"min_max_cluster_sums_{current_date}_thresh_{threshold_sum_t}.csv\"\n",
    ")\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "cluster_sums_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Saved min and max cluster sums to {output_csv_path}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c9e1512e14c22663",
   "metadata": {},
   "source": [
    "# test load csv file\n",
    "# Load the CSV file into a DataFrame\n",
    "cluster_sums_df = pd.read_csv(output_csv_path)\n",
    "# Verify the data\n",
    "print(cluster_sums_df.head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bf07cde0f890be18",
   "metadata": {},
   "source": [
    "\n",
    "## veiw the min and max cluster sums distribution"
   ]
  },
  {
   "cell_type": "code",
   "id": "54b37b4d016b4fa5",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Drop rows with NaN values\n",
    "cluster_sums_df = cluster_sums_df.dropna(subset=['MinClusterSum', 'MaxClusterSum'])\n",
    "\n",
    "\n",
    "# Assume you have a DataFrame named cluster_sums_df\n",
    "# with columns 'MinClusterSum' and 'MaxClusterSum'\n",
    "\n",
    "# Create a figure with 1 row and 2 columns\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Plot histogram of MinClusterSum\n",
    "sns.histplot(data=cluster_sums_df, x='MinClusterSum', kde=True, ax=axs[0], color='blue')\n",
    "axs[0].set_title('Distribution of Min Cluster Sum')\n",
    "\n",
    "# Plot histogram of MaxClusterSum\n",
    "sns.histplot(data=cluster_sums_df, x='MaxClusterSum', kde=True, ax=axs[1], color='red')\n",
    "axs[1].set_title('Distribution of Max Cluster Sum')\n",
    "\n",
    "# Adjust layout and show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8d6bdf5469eef338",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "alpha = 0.01\n",
    "lower_pct = 100 * (alpha / 2)      # 2.5 percentile\n",
    "upper_pct = 100 * (1 - alpha / 2)  # 97.5 percentile\n",
    "\n",
    "# Calculate thresholds for MinClusterSum\n",
    "min_lower_thresh = np.percentile(cluster_sums_df['MinClusterSum'], lower_pct)\n",
    "min_upper_thresh = np.percentile(cluster_sums_df['MinClusterSum'], upper_pct)\n",
    "\n",
    "# Calculate thresholds for MaxClusterSum\n",
    "max_lower_thresh = np.percentile(cluster_sums_df['MaxClusterSum'], lower_pct)\n",
    "max_upper_thresh = np.percentile(cluster_sums_df['MaxClusterSum'], upper_pct)\n",
    "\n",
    "print(f\"MinClusterSum thresholds (p=0.05, two-sided): {min_lower_thresh} to {min_upper_thresh}\")\n",
    "print(f\"MaxClusterSum thresholds (p=0.05, two-sided): {max_lower_thresh} to {max_upper_thresh}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "80ad8648a5abaa13",
   "metadata": {},
   "source": [
    "'''\n",
    "learning \n",
    "\n",
    "how df influence the t distribution?\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import t\n",
    "\n",
    "# Degrees of freedom to compare\n",
    "dfs = [60000, 70000]\n",
    "\n",
    "# Create an x-axis from -5 to 5\n",
    "x = np.linspace(-5, 5, 400)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for df in dfs:\n",
    "    # Get the t-distribution PDF for this df\n",
    "    pdf = t.pdf(x, df)\n",
    "    plt.plot(x, pdf, label=f\"df={df}\")\n",
    "\n",
    "# Also include the standard normal distribution for comparison\n",
    "from scipy.stats import norm\n",
    "plt.plot(x, norm.pdf(x), label=\"Normal (df=∞)\", linestyle='--', color='black')\n",
    "\n",
    "plt.title(\"T-Distribution with Different Degrees of Freedom\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Probability Density\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1428773ab7eff05b",
   "metadata": {},
   "source": [
    "'''\n",
    "instant code \n",
    "\n",
    "find files' name and delete files who have been updated.\\\n",
    "\n",
    "'''\n",
    "\n",
    "import os\n",
    "\n",
    "directory = r'H:\\lyw\\sub28'\n",
    "\n",
    "# Print only file names\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        print(file)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5782e2aab5e741e4",
   "metadata": {},
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Define the directory path\n",
    "directory = r'H:\\lyw\\sub28'\n",
    "\n",
    "# Initialize a list to store numbers\n",
    "numbers = []\n",
    "\n",
    "# Walk through the directory\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        # Check if the file matches the pattern\n",
    "        if re.match(r'summary_results_permutation_\\d+\\.csv', file):\n",
    "            # Extract the number using regex\n",
    "            num = re.search(r'\\d+', file).group()\n",
    "            numbers.append(int(num))\n",
    "\n",
    "# Print the numbers\n",
    "print(\"Extracted numbers:\", numbers)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cb71a50e30d21ce",
   "metadata": {},
   "source": [
    "# Define the full range of numbers from 1 to 1000\n",
    "full_range = set(range(1, 1001))\n",
    "\n",
    "# Example extracted numbers (as a list)\n",
    "extracted_numbers = numbers\n",
    "\n",
    "# Convert the list to a set for set operations\n",
    "extracted_numbers_set = set(extracted_numbers)\n",
    "\n",
    "# Find missing numbers\n",
    "missing_numbers = sorted(full_range - extracted_numbers_set)\n",
    "\n",
    "# Display the missing numbers\n",
    "print(\"Missing numbers:\", missing_numbers)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6d8af2bd8d5c0e85",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne12",
   "language": "python",
   "name": "mne12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
